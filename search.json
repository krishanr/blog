[
  {
    "objectID": "posts/2025-08-10-transformer-architecture.html",
    "href": "posts/2025-08-10-transformer-architecture.html",
    "title": "Transformer Architecture",
    "section": "",
    "text": "This a brief review of the transformer architecture following Lecture 17 - Attention by Justin Johnson, Chapter 9 - The Transformer, and the Transformer architecture lecture in Stanford’s CS336. It will cover attention, the transformer block, and the decoder-only transformer architecture, from the early 2020s to recent improvements. Other useful references include Transformers from scratch by Peter Bleom, Harvard’s The annotated transformer, Lilian Weng’s blog on The Transformer V2, Xavier Amatriain’s Transformer models: an introduction and catalog — 2023 Edition, and Sebastian Raschka’s posts on the transformer, including The Big LLM Architecture Comparison."
  },
  {
    "objectID": "posts/2025-08-10-transformer-architecture.html#attention",
    "href": "posts/2025-08-10-transformer-architecture.html#attention",
    "title": "Transformer Architecture",
    "section": "Attention",
    "text": "Attention\nFirst \\(L\\) tokens are embedded into a vector space of dimension \\(d_{model}\\) to give a matrix\n\\[\n  X : L \\times d_{model}\n\\]\nThe dimensions \\(L\\) and \\(d_{model}\\) are important for all transformer architectures. \\(L\\) is the context length of the model controlling the length of the text sequences visible to the model, and \\(d_{model}\\) is the embedding dimension controlling the capacity of the model.\nThe attention mechanism uses the following weight matricies:\n\\[W_K : d_{model} \\times d_{k}\\] \\[W_Q : d_{model} \\times d_{k}\\] \\[W_V : d_{model} \\times d_{v}\\]\nThe query, key, and value matrices are then defined by:\n\\[Q = X W_Q\\] \\[K = X W_K\\] \\[V = X W_V\\]\nThe similarity matrix \\(E\\) is defined by:\n\\[E = \\frac{Q K^T}{\\sqrt{d_k}} : L \\times L\\]\nThe softmax is then applied over the key dimension (dimension \\(1\\)) to give the attention matrix \\(A\\):\n\\[A = \\operatorname{softmax}(E, \\dim =1) : L \\times L\\]\nThe output matrix \\(Y\\) is then defined by:\n\\[Y = A V : L \\times d_{v}\\]\nThis gives one attention head \\(h_i\\) for \\(i = 1,..,h\\). Specifically,\n\\[ \\operatorname{head}_i = \\operatorname{Attention}(X W_i^Q,X W_i^K, X W_i^V)  \\]\nThe final output is\n\\[ \\operatorname{MultiHead}(Q,K,V) = \\operatorname{Concat}(\\operatorname{head}_1,...,\\operatorname{head}_h) W^O \\]\nwhere \\(\\operatorname{Concat}\\) is done along the embedding dimension and \\(W_O\\) has shape \\(h d_v \\times d_{model}\\).\nThe \\(\\operatorname{Attention}\\) block is the essence of the transformer architecture, and is largely unchanged to this day. Its main limitation is that it has computational complexity \\(O(L^2 d_{model})\\) which makes it difficult to evaluate for long contexts. Much work has gone into reducing the \\(L^2\\) complexity, including Grouped Query Attention, which is common today.\n\nCross-attention\nThis layer was used in the original transformer paper, to connect the encoder output with the decoder branch, and is still used today, for example, to make multimodal LLMs such as Llama 3.2.\n\\(X_1 : L_1 \\times d_{model}\\) (from the decoder)\n\\(X_2 : L_2 \\times d_{model}\\) (from the encoder)\nNotice that the embedding dimension for \\(X_1\\) and \\(X_2\\) are the same. The weight matrices are:\n\\[W_K : d_{model} \\times d_{k}\\] \\[W_Q : d_{model} \\times d_{k}\\] \\[W_V : d_{model} \\times d_{v}\\]\nThe query, key, and value matrices in cross-attention are:\n\\[Q = X_1 W_Q\\] \\[K = X_2 W_K\\] \\[V = X_2 W_V\\]\nThe main difference now is that the similarity matrix \\(E\\) defined earlier has dimension \\(L_1 \\times L_2\\).\n\n\nSelf-attention in vector form\nThe vector formulation of attention is easier to understand. First define the query, key, and value vectors as follows:\n\\(q_m\\) are query vectors for \\(m=1,...,L\\)\n\\(k_n\\) are key vectors for \\(n=1,...,L\\)\n\\(v_n\\) are value vectors for \\(n=1,...,L\\)\nThen the \\(m,n\\) entry of the attention matrix is given by:\n\\[a_{m,n} = \\frac{\\exp\\left(\\frac{q_m^T k_n}{\\sqrt{d_k}}\\right)}{\\sum_{j=1}^N \\exp\\left(\\frac{q_m^T k_j}{\\sqrt{d_k}}\\right)}\\]"
  },
  {
    "objectID": "posts/2025-08-10-transformer-architecture.html#transformer-block",
    "href": "posts/2025-08-10-transformer-architecture.html#transformer-block",
    "title": "Transformer Architecture",
    "section": "Transformer block",
    "text": "Transformer block\nTransformer block (equations move forward from top to bottom) mapping \\(h_{i-1} \\in \\mathbb{R}^{L\\times d_{model}}\\) to \\(h_{i} \\in \\mathbb{R}^{L\\times d_{model}}\\) is: \nThe diagram is read from bottom to top, but the following equations are read from top to bottom.\n\\[\\mathbf{T}^1_i = \\text{LayerNorm}(\\mathbf{h_{i-1}})\\]\n\\[\\mathbf{T}^2_i = \\text{MultiHeadAttention}(\\mathbf{T}^1_i)\\]\n\\[\\mathbf{T}^3_i = \\mathbf{T}^2_i + \\mathbf{h_{i-1}}\\]\n\\[\\mathbf{T}^4_i = \\text{LayerNorm}(\\mathbf{T}^3_i)\\]\n\\[\\mathbf{T}^5_i = \\text{FFN}(\\mathbf{T}^4_i)\\]\n\\[\\mathbf{h_{i}} = \\mathbf{T}^5 + \\mathbf{T}^3_i\\]\nThe primary difference between the decoder-only transformer block and the encoder-only transformer block is that the decoder-only block uses masked (causal) self-attention instead of the full self-attention. The simplest way to describe it is that the output matrix \\(Y\\) now takes the form\n\\[\n  Y_i = \\sum_{j \\leq i} A_{ij} V_j\n\\]\nThis is because generating the probability distribution for token \\(i+1\\) requires only tokens \\(1\\) up to \\(i\\).\nThe feed forward layers are defined by:\n\\[\n    FFN(X) = ReLU(X W + b_1)W_2 + b_2\n\\]\nIn conclusion, note that an important property of the \\(\\operatorname{TransformerBlock}\\) is that the interaction between the tokens comes only through \\(\\operatorname{MultiHeadAttention}\\), while the \\(\\operatorname{FFN}\\) layer acts on each token separately. This is because self-attention is permutation equivariant as a function on the tokens, which is why position embeddings are added to the embedded tokens.\n\nContemporary improvements\n\nMost architectures put the normalization layer outside the residual stream, right before the \\(\\operatorname{MultiHeadAttention}\\) or \\(\\operatorname{FFN}\\) layer (there’s evidence that this improves training performance and stability)\n\\(\\operatorname{LayerNorm}\\) has been replaced by \\(\\operatorname{RMSNorm}\\) (\\(\\operatorname{RMSNorm}\\) has fewer operations, thereby reducing runtime without reducing performance)\nThe bias terms in the feed-forward layers are set to zero (similar reason to \\(\\operatorname{RMSNorm}\\) regarding memory and optimization stability). So (letting \\(\\sigma\\) denote the nonlinearity in the \\(\\operatorname{FFN}\\) layer)\n\\[ FFN(X) = \\sigma(X W )W_2 \\]\n\\(\\operatorname{Dropout}\\) is no longer used\n\\(ReLU\\) is replaced with \\(SwiGLU\\)\nSome models do \\(\\operatorname{MultiHeadAttention}\\) and \\(\\operatorname{FFN}\\) in parallel instead of sequentially\n\\(\\sin\\) positional embeddings have been replaced with \\(\\operatorname{RoPE}\\) embeddings\n\\(\\operatorname{MultiHeadAttention}\\) has given way to \\(\\operatorname{GroupedQueryAttention}\\)\nA more memory efficient implementation of attention called \\(\\operatorname{FlashAttention}\\) is now widely used\n\nAditionally, many large langaguage models now use mixture of experts in place of the feed-forward layer, which allows for larger total parameter counts per FLOP. For example, the recent Kimi K2 model is a mixture of experts model with 1 trillion total parameters. Another motivation for mixture of experts is that 90% of parameters in Google’s 540B PaLM model were in the FFN layers, so these layers had to be made more efficient."
  },
  {
    "objectID": "posts/2025-08-10-transformer-architecture.html#decoder-only-transformer-architecture",
    "href": "posts/2025-08-10-transformer-architecture.html#decoder-only-transformer-architecture",
    "title": "Transformer Architecture",
    "section": "Decoder only transformer architecture",
    "text": "Decoder only transformer architecture\nThe decoder only transformer was described in the original GPT paper for natural language understanding tasks. Ever since, it’s been the go-to architecture for building large language models.\nTo describe the decoder-only transformer architecture, first let \\(X = [x_1,...,x_{L}]\\) be the sequence of tokens used to predict the probability vector for token \\(L+1\\). The decoder is autoregressive, since it only uses tokens \\(1\\) up to \\(L\\) to predict token \\(L+1\\), instead of subsequent tokens \\(L+2\\), \\(L+3\\) etc. Let \\(h_0 = X\\) be the zeroth transformer layer, then there are \\(n\\) transformer layers, \\(h_1,...,h_n\\) thereafter. Furthermore, let \\(W_e\\) be the embedding matrix and \\(W_p\\) be the position embedding matrix. Then\n\n\\(h_0 = X W_e + W_p\\)\n\\(h_l = \\operatorname{TransformerBlock}(h_{l-1})\\) using causal self-attention for \\(l \\in [ 1,\\dotsc,n ]\\)\n\\(P(x_{L+1} \\: | \\: x_1,...,x_{L}) = \\operatorname{softmax}(h_n^L W_e^T)\\) is the probability distribution for the next token, token \\(L+1\\).\n\nNote that \\(W_e\\) is learnt (while \\(W_p\\) is usually not) so that \\(W_e^T\\) maps back to the vocabulary space.\nFor supervised fine-tuning to predict a label \\(y\\), the linear layer \\(W_e^T\\) is replaced with \\(W_y\\) giving\n\\[\n  P(y \\: | \\: x_1,...,x_{L}) = \\operatorname{softmax}(h_n^L W_y)\n\\]"
  },
  {
    "objectID": "posts/2025-08-10-transformer-architecture.html#encoder-only-transformer-architecture",
    "href": "posts/2025-08-10-transformer-architecture.html#encoder-only-transformer-architecture",
    "title": "Transformer Architecture",
    "section": "Encoder only transformer architecture",
    "text": "Encoder only transformer architecture\nThe encoder only transformer was described in the original BERT paper for natural language understanding tasks. More recently, they are still used for text embedding models such as E5, and vision encoders based on VIT.\nHere we have a fixed context window of tokens \\(X =  [x_1,...,x_{L}]\\) which we want to analyze. The encoder continously maps this sequence to another sequence \\([z_1,...,z_{L}]\\). The transformer is then the same as the decoder-only transformer, except: (1) full self-attention is used in each \\(\\operatorname{TransformerBlock}\\) instead of masked self-attention, and (2) there is no final classification head to predict a probability distribution. Instead, a variable number of classification heads can be added to the final layer \\(h_n\\) depending on the pre-training or fine-tuning task. In conclusion, full self-attention gives each layer access to all tokens, which is better suited for natural language understanding tasks."
  },
  {
    "objectID": "posts/2025-08-10-transformer-architecture.html#encoder-decoder-transformer-architecture",
    "href": "posts/2025-08-10-transformer-architecture.html#encoder-decoder-transformer-architecture",
    "title": "Transformer Architecture",
    "section": "Encoder-decoder transformer architecture",
    "text": "Encoder-decoder transformer architecture\nThe encoder-decoder transformer was used in the original transformer paper Attention is all you need, but is less prevalent today. It combines the encoder and decoder architectures via the cross-attention mechanism. See the original paper for more details."
  },
  {
    "objectID": "posts/2025-08-10-transformer-architecture.html#conclusion",
    "href": "posts/2025-08-10-transformer-architecture.html#conclusion",
    "title": "Transformer Architecture",
    "section": "Conclusion",
    "text": "Conclusion\nHere are the parameter choices from the original Transformer architecture designed for language translation (which used an encoder-decoder architecture):\n\n\\(d_{model} = 512\\)\n\\(d_k = d_v = 64\\)\n\\(h = 8\\)\n\\(d_{ff} = 2048\\)\n\\(n = 6\\) for encoder-decoder each\n\nMore examples (source Johnson lecture 17):\n\n\n\nEarly transformer architectures\n\n\nMany more variations of the transformer have been made since, a sample of which are depicted below (source):\n\n\n\nTransformer Zoo\n\n\nThe above diagram color codes the different transformer families, with pink denoting decoder-only transformers based on GPT, and green denoting encoder-only transformers based on BERT. Finally, see The Big LLM Architecture Comparison for a more recent survey of transformer architectures."
  },
  {
    "objectID": "posts/2025-08-10-transformer-architecture.html#references",
    "href": "posts/2025-08-10-transformer-architecture.html#references",
    "title": "Transformer Architecture",
    "section": "References",
    "text": "References\n\nAcademic Papers and Publications\n\nAttention is all you need - Original Transformer paper\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - BERT paper\nImproving Language Understanding by Generative Pre-Training - Original GPT paper\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - Vision Transformer (ViT) paper\nText Embeddings by Weakly-Supervised Contrastive Pre-training - E5 text embedding model\n\n\n\nLectures and Course Materials\n\nLecture 17 - Attention - Justin Johnson, University of Michigan\nChapter 9 - The Transformer - Stanford NLP Course\nTransformer Architecture - Stanford CS336 Lecture 3\nMixture of Experts Stanford CS336 Lecture 4\nTransformer Block Slides - Stanford\n\n\n\nBlog Posts and Tutorials\n\nTransformers from scratch - Peter Bloem\nThe Annotated Transformer - Harvard NLP\nThe Transformer Family V2 - Lilian Weng’s Blog\nTransformer models: an introduction and catalog — 2023 Edition - Xavier Amatriain\nThe Big LLM Architecture Comparison - Sebastian Raschka\nUnderstanding Multimodal LLMs - Sebastian Raschka\n\n\n\nImages and Diagrams\n\nTransformer Zoo Diagram - Xavier Amatriain’s Blog"
  },
  {
    "objectID": "posts/2022-09-19-opencv-contours.html",
    "href": "posts/2022-09-19-opencv-contours.html",
    "title": "Contours in OpenCV",
    "section": "",
    "text": "!pip install opencv-python==4.5.5.64\n!pip install watermark\nThe official tutorial on contours in OpenCV can be found here. We’ll summarize some important methods discussed in this notebook, using the image below as an example.\nimport cv2 as cv\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef display_image(img):\n    fig, ax = plt.subplots(figsize=(16,16))\n    ax.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))\n    plt.show() \n\nimg = cv.imread(\"FindingContours.png\")\ndisplay_image(img)\nNormally before finding contours in an image the image has to be binarized using a threshold algorithm, or canny edge detection. But in this case the image is already binarized, as the following calculation shows.\nprint(\"Shape: \", img.shape)\n# The image pixels only take 2 values, hence it's binarized.\nprint(\"Values: \", np.unique(img))\n\nShape:  (578, 1132, 3)\nValues:  [  0 255]\nWe’ll first convert the image to gray scale.\ngray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\ngray.shape\n\n(578, 1132)\nNow find contours using the cv.CHAIN_APPROX_NONE method. This requires more memory but the following methods tend to be more robust (this includes convexity checking and orientation finding methods).\ncontours, _ = cv.findContours(gray, cv.RETR_LIST, cv.CHAIN_APPROX_NONE)\nlen(contours)\n\n10\nLabel the contours so we can refer to them later. The first method we’ll use below is contourArea to filter out noise. We’ll also use moments to calculate the contours centroid.\nmimg = img.copy()\n# Go through all non-trivial contours and label them.\nfor i, contour in enumerate(contours):\n    area = cv.contourArea(contour)\n\n    if area &lt; 1e2 or area &gt; 1e5:\n        continue\n\n    moments = cv.moments(contour)\n    cx, cy = int(moments[\"m10\"]/moments[\"m00\"]), int(moments[\"m01\"]/moments[\"m00\"])\n\n    mimg = cv.putText(mimg, str(i), (cx,cy), cv.FONT_HERSHEY_COMPLEX, \n                      1, (255,0,0), 2, cv.LINE_AA)\n\n# Replace -1 with the contour index to draw a specific contour.                \ncv.drawContours(mimg, contours, -1, (0,255,0), 3)\ndisplay_image(mimg)\nNow we’ll calculate several contour properties, including convexity, area, perimeter and angle. An important detail to keep in mind is that image coordinates have clockwise orientation (in contrast with Cartesian coordinates which are counter-clockwise). So for example, an angle of 10 degrees, lies in the quadrant with positive \\(x\\) and \\(y\\) values. See this tutorial for further details.\nfor i, contour in enumerate(contours):\n    area = cv.contourArea(contour)\n\n    if area &lt; 1e2 or area &gt; 1e5:\n        continue\n\n    # isContourConvex doesn't work so well and depends on the chain approx\n    # method.\n    isconvex = cv.isContourConvex(contour)\n    perimeter = cv.arcLength(contour, True)\n    # We'll use the angle returned by fitEllipse\n    _, _, angle = cv.fitEllipse(contour)   \n    print(f\"{i} IsConvex: {isconvex}, Area: {area:.2f}, Perimeter: {perimeter:.2f}, Angle: {angle:.2f}\")\n\n0 IsConvex: False, Area: 5852.00, Perimeter: 306.00, Angle: 90.00\n1 IsConvex: False, Area: 7104.50, Perimeter: 485.49, Angle: 19.44\n2 IsConvex: False, Area: 5920.00, Perimeter: 310.84, Angle: 45.00\n3 IsConvex: False, Area: 10290.00, Perimeter: 382.68, Angle: 90.00\n4 IsConvex: False, Area: 7119.50, Perimeter: 343.81, Angle: 90.00\n5 IsConvex: False, Area: 6518.00, Perimeter: 382.74, Angle: 40.76\n6 IsConvex: False, Area: 5357.50, Perimeter: 358.21, Angle: 0.00\n7 IsConvex: False, Area: 3375.00, Perimeter: 276.65, Angle: 90.00\n8 IsConvex: False, Area: 7056.00, Perimeter: 329.76, Angle: 90.00\n9 IsConvex: False, Area: 15776.50, Perimeter: 492.66, Angle: 27.55\nBelow we’ll use boundingRect to draw bounding boxes around the contours.\nfor contour in contours:\n    x,y,w,h = cv.boundingRect(contour)\n\n    cv.rectangle(mimg, (x,y), (x+w, y+h), (0,0,255),2)\n\ndisplay_image(mimg)\nNow we’ll consider another method called minAreaRect, which calculates the rectangle enclosing the contour with minimal area.\nfor contour in contours:\n    area = cv.contourArea(contour)\n\n    if area &lt; 1e2 or area &gt; 1e5:\n        continue\n\n    # rect is the following tuple: ( center (x,y), (width, height), angle of rotation )\n    rect = cv.minAreaRect(contour)\n    # Need to get verticies to draw the rectangle.\n    box = cv.boxPoints(rect)\n    box = np.int0(box)\n\n    cv.drawContours(mimg, [box], 0, (255,0,0), 2)\n\ndisplay_image(mimg)\nAt this point we’ve seen two methods that give an orientation of a contour. The first was fitEllipse, and then minAreaRect. Finally we’ll use PCACompute21 (see Introduction to PCA) for a third method for obtaining the contour orientation and compare it with the former methods.\nfor i, contour in enumerate(contours):\n    area = cv.contourArea(contour)\n\n    if area &lt; 1e2 or area &gt; 1e5:\n        continue\n        \n    print(f\"Contour {i}\")\n    _,_, angle_minor = cv.fitEllipse(contour)\n    print(f\"fitEllipse angle: {angle_minor:.2f}\")\n    _, _, angle_rect = cv.minAreaRect(contour)  \n    print(f\"minAreaRect angle: {angle_rect:.2f}\") \n\n    mean = np.empty((0))\n    mean, eigenvectors, eigenvalues = cv.PCACompute2(contour.squeeze().astype(np.float64), mean)\n\n    # Compare with angles from PCA.\n    print(\"PCA angles\", [ round(num, 2) for num in cv.phase(eigenvectors[:,0], eigenvectors[:,1], angleInDegrees=True).reshape(-1)] )\n    print(\"\")\n\nContour 0\nfitEllipse angle: 90.00\nminAreaRect angle: 90.00\nPCA angles [0.0, 90.0]\n\nContour 1\nfitEllipse angle: 98.17\nminAreaRect angle: 19.44\nPCA angles [10.81, 100.81]\n\nContour 2\nfitEllipse angle: 0.04\nminAreaRect angle: 45.00\nPCA angles [89.99, 359.99]\n\nContour 3\nfitEllipse angle: 0.09\nminAreaRect angle: 90.00\nPCA angles [90.05, 0.05]\n\nContour 4\nfitEllipse angle: 0.15\nminAreaRect angle: 90.00\nPCA angles [90.38, 0.38]\n\nContour 5\nfitEllipse angle: 0.03\nminAreaRect angle: 40.76\nPCA angles [90.02, 0.02]\n\nContour 6\nfitEllipse angle: 116.06\nminAreaRect angle: 0.00\nPCA angles [27.48, 117.48]\n\nContour 7\nfitEllipse angle: 0.01\nminAreaRect angle: 90.00\nPCA angles [90.05, 0.05]\n\nContour 8\nfitEllipse angle: 90.00\nminAreaRect angle: 90.00\nPCA angles [0.0, 90.0]\n\nContour 9\nfitEllipse angle: 109.28\nminAreaRect angle: 27.55\nPCA angles [17.31, 107.31]\nNote that the PCA angles are the angles made by the semi-major and semi-minor axis of the PCA ellipse about the x-axis. The above calculations suggest that fitEllipse returns the angle made by the semi-minor axis of the PCA ellipse (also see here). Due to this simple geometric interpretation, we use the angle returned by fitEllipse to represent the orientation of the contour.\n%load_ext watermark\n%watermark --iversions -v\n\nPython implementation: CPython\nPython version       : 3.9.7\nIPython version      : 7.31.0\n\ncv2       : 4.5.5\nnumpy     : 1.19.5\nmatplotlib: 3.5.1"
  },
  {
    "objectID": "posts/2022-09-19-opencv-contours.html#footnotes",
    "href": "posts/2022-09-19-opencv-contours.html#footnotes",
    "title": "Contours in OpenCV",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that the contour was converted to floats before passing it to PCACompute2. This is a consistent pattern throughout opencv-python that the appropriate data type has to be used.↩︎"
  },
  {
    "objectID": "posts/2022-08-25-pet_segmentation.html",
    "href": "posts/2022-08-25-pet_segmentation.html",
    "title": "Pet Segmentation",
    "section": "",
    "text": "In this example, we will build an image segmentation model to segment the 3 different classes in the Oxford-IIIT Pet Segmentation Dataset.\nWe’ll use Segmentation Models PyTorch which was introduced in an earlier post on Surface Defect Segmentation, but in this post we focus on Torchmetrics, which is a new library that has many metrics for classification/segmentation in Pytorch. Torchmetrics\nTorchmetrics was already introduced for Pet Breed Classification, but in this post we’ll describe the mdmc_average parameter which is relevant for higher dimensional image data.\nFirst we’ll install Torchmetrics with PyTorch Lightning below.\n!pip install pytorch_lightning\n!pip install -U git+https://github.com/qubvel/segmentation_models.pytorch\n!pip install watermark\nimport os\nimport random\nimport collections\nimport numpy as np\nimport torch\nimport torchvision\nimport matplotlib.pyplot as plt\n\n# for plotting\nplt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\nplt.rcParams[\"font.size\"] = 16\n\nfrom pytorch_lightning import LightningModule, Trainer, seed_everything\nimport segmentation_models_pytorch as smp\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import random_split\n\nseed_everything(7)\n\nGlobal seed set to 7\n\n\n7"
  },
  {
    "objectID": "posts/2022-08-25-pet_segmentation.html#exploratory-data-analysis",
    "href": "posts/2022-08-25-pet_segmentation.html#exploratory-data-analysis",
    "title": "Pet Segmentation",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWe’ll first look at some image/mask pairs in the dataset and basic dataset statistics. To do this, we’ll resize the images to a standard size of \\(224 \\times 224\\).\n\ndef transforms(image,target):\n    image, target = TF.resize(image,(256,256)), TF.resize(target,(256,256))\n    image, target = TF.center_crop(image,224), TF.center_crop(target, 224)\n    # Shift the indicies so that they are from 0,...,num_classes-1\n    return TF.to_tensor(image), 255*TF.to_tensor(target) - 1\n\n\n# set download to False after the dataset is downloaded.\nvis_dataset = torchvision.datasets.OxfordIIITPet(root=\"./data\", split=\"trainval\", target_types=\"segmentation\",transforms=transforms,download=True)\n\nDownloading https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz to data/oxford-iiit-pet/images.tar.gz\n\n\n\n\n\nExtracting data/oxford-iiit-pet/images.tar.gz to data/oxford-iiit-pet\nDownloading https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz to data/oxford-iiit-pet/annotations.tar.gz\n\n\n\n\n\nExtracting data/oxford-iiit-pet/annotations.tar.gz to data/oxford-iiit-pet\n\n\nAs we shall see below, the segmentation masks have 3 labels (see the Cats and Dogs original paper):\n\nPet Body\nBackground\nAmbiguous region (between body and background)\n\n\nfor i in range(5):\n    sample_img, sample_msk = vis_dataset[random.choice(range(len(vis_dataset)))]\n    plt.subplot(1,2,1)\n    plt.title(\"Image\")\n    plt.axis(\"off\")\n    plt.imshow(sample_img.permute([1,2,0]))\n    plt.subplot(1,2,2)\n    plt.title(\"Mask\")\n    plt.axis(\"off\")\n    plt.imshow(sample_msk.permute([1,2,0]).squeeze())\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we’ll calculate the mask statistics.\n\n# Use a dataloader to speed up the loading of masks.\nvis_dataloader = torch.utils.data.DataLoader(vis_dataset, shuffle=False, batch_size=16, num_workers=os.cpu_count())\npixel_counts = collections.defaultdict(int)\n\nfor _, mask in vis_dataloader:\n    labels, counts = np.unique(mask.numpy(),return_counts=True)\n    labels = list(map(int, labels))\n    \n    for label, count in zip(labels,counts):\n        pixel_counts[label] += count\n        \n# Work with normalized counts\npixel_counts = np.array(list(pixel_counts.values()))/sum(pixel_counts.values())\n\nAs the figure below shows, our dataset is mildly imbalanced. Thus as mentioned in the Surface Defect Segmentation post, it makes sense to experiment with different loss functions offered by the segmentation models library.\n\nfig, ax = plt.subplots(figsize=(8,5))    \nax.barh(range(len(pixel_counts)), pixel_counts)\nwidth=0.15\nind = np.arange(3)\nax.set_yticks(ind+width/2)\nax.set_yticklabels([\"Body\", \"Background\", \"Border\"], minor=False)\nplt.xlabel(\"Number of pixels per class\")\nplt.title(\"Distribution of pixel labels\")\nplt.show()"
  },
  {
    "objectID": "posts/2022-08-25-pet_segmentation.html#training",
    "href": "posts/2022-08-25-pet_segmentation.html#training",
    "title": "Pet Segmentation",
    "section": "Training",
    "text": "Training\nDefine data augmentations to use with the train dataset. More data augmentations are possible with the albumentations library.\n\ndef train_transforms(image,target):\n    # Only horizontal flips\n    if random.random() &lt; 0.5:\n        image = TF.hflip(image)\n        target = TF.hflip(target)\n        \n    image, target = TF.resize(image,(256,256)), TF.resize(target,(256,256))\n    image, target = TF.center_crop(image,224), TF.center_crop(target, 224)\n    # Shift the indicies so that they are from 0,...,num_classes-1\n    return TF.to_tensor(image), 255*TF.to_tensor(target) - 1\n\n\ndef val_transforms(image,target):\n    image, target = TF.resize(image,(256,256)), TF.resize(target,(256,256))\n    image, target = TF.center_crop(image,224), TF.center_crop(target, 224)\n    # Shift the indicies so that they are from 0,...,num_classes-1\n    return TF.to_tensor(image), 255*TF.to_tensor(target) - 1\n\n\n# Set the download parameter to False after the datasets are downloaded.\ntrain_dataset = torchvision.datasets.OxfordIIITPet(root=\"./data\", split=\"trainval\", target_types=\"segmentation\",transforms=train_transforms,download=False)\nval_dataset = torchvision.datasets.OxfordIIITPet(root=\"./data\", split=\"test\", target_types=\"segmentation\",transforms=train_transforms,download=False)\n\n\nprint('Length of train dataset: ', len(train_dataset))\nprint('Length of validation dataset: ', len(val_dataset))\n\nLength of train dataset:  3680\nLength of validation dataset:  3669\n\n\n\nnum_classes = 3\nBATCH_SIZE = 16\n\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\nval_dataloader = torch.utils.data.DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\n\nNow we’ll subclass the LightningModule to create and train the model. The code below is similar to that in Pet Breed Classification and Surface Defect Segmentation, with the main difference being the metrics we define below.\nWe’ll use the Accuracy and F1Score with torchmetrics to measure the performance of our model. The main difference from the pet breed classification example (which describes the average parameter) is that now we have to use the mdmc_average parameter to reduce the extra image dimensions, \\(H\\) and \\(W\\). We shall use mdmc_average=global, which is described in greater detail below.\nFor a given batch of data of shape \\([B, C,H,W]\\), the option mdmc_average=global collapses the data into shape \\([B\\times H \\times W, C]\\) and then calculates the F1Score as for multiclass classifiers. The option mdmc_average=samplewise on the other hand calculates the F1score for each of the \\(B\\) samples and each of the \\(C\\) classes, and then averages over the sample and class dimensions (cf. F1Score). The logic is similar for other metrics like the Dice score for example. This will be elaborated in an upcoming post, giving comparisons with the metrics in segmentation models pytorch, and recommendations for practical usage.\n\nimport torch.nn as nn\nfrom torchmetrics import MetricCollection, Accuracy, F1Score\nfrom torch.nn import functional as F\n\nclass PetModel(LightningModule):\n    \n    def __init__(self, arch, encoder_name, learning_rate, num_classes, loss=\"DiceLoss\", **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n        self.example_input_array = torch.zeros((BATCH_SIZE, 3, 224,224))\n        \n        # Setup the model.\n        self.model = smp.create_model(\n            arch, encoder_name=encoder_name, encoder_weights = \"imagenet\", in_channels=3, classes=num_classes, **kwargs\n        )\n        \n        # Setup the losses.\n        if loss == \"CrossEntropy\":\n            self.loss = nn.CrossEntropyLoss()\n        else:\n            self.loss = smp.losses.DiceLoss(smp.losses.MULTICLASS_MODE, from_logits=True)\n        \n        # Setup the metrics.\n        self.train_metrics = MetricCollection({\"train_acc\" : Accuracy(num_classes=num_classes, average=\"micro\",mdmc_average=\"global\"),\n                                               \"train_f1\" : F1Score(num_classes=num_classes,average=\"weighted\",mdmc_average=\"global\")})\n        self.val_metrics = MetricCollection({\"val_acc\" : Accuracy(num_classes=num_classes, average=\"micro\",mdmc_average=\"global\"),\n                                               \"val_f1\" : F1Score(num_classes=num_classes,average=\"weighted\",mdmc_average=\"global\")})\n        self.test_metrics = MetricCollection({\"test_acc\" : Accuracy(num_classes=num_classes, average=\"micro\",mdmc_average=\"global\"),\n                                               \"test_f1\" : F1Score(num_classes=num_classes,average=\"weighted\",mdmc_average=\"global\")})\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def training_step(self, batch, batch_idx):\n        images, targets = batch\n        #TODO: do this at dataset preparation.\n        targets = targets.squeeze().long()\n        \n        logits_mask = self.forward(images)\n        loss = self.loss(logits_mask, targets)\n        preds = torch.softmax(logits_mask, dim=1)\n        \n        self.train_metrics(preds, targets)\n        \n        self.log(\"train_acc\", self.train_metrics[\"train_acc\"], prog_bar=True)\n        self.log(\"train_f1\", self.train_metrics[\"train_f1\"], prog_bar=True)        \n        self.log(\"train_loss\", loss, prog_bar=True)        \n        \n        return loss\n    \n    def evaluate(self, batch, stage=None):\n        images, targets = batch\n        targets = targets.squeeze().long()\n        \n        logits_mask = self.forward(images)\n        loss = self.loss(logits_mask, targets)\n        preds = torch.softmax(logits_mask, dim=1)\n        \n        if stage == \"val\":\n            self.val_metrics(preds,targets)\n            \n            self.log(\"val_acc\", self.val_metrics[\"val_acc\"], prog_bar=True)\n            self.log(\"val_f1\", self.val_metrics[\"val_f1\"], prog_bar=True)        \n            self.log(\"val_loss\", loss, prog_bar=True)  \n        elif stage == \"test\":\n            self.test_metrics(preds,targets)\n            \n            self.log(\"test_acc\", self.test_metrics[\"test_acc\"], prog_bar=True)\n            self.log(\"test_f1\", self.test_metrics[\"test_f1\"], prog_bar=True)        \n            self.log(\"test_loss\", loss, prog_bar=True)  \n            \n    def validation_step(self, batch, batch_idx):\n        return self.evaluate(batch, \"val\")\n    \n    def test_step(self, batch, batch_idx):\n        return self.evaluate(batch, \"test\")\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(params=self.parameters(), lr=self.hparams.learning_rate)\n\nWe’ll start off with the UNET architecture using a resnet34 backbone. Other options include using the DeepLabV3 architecture and the RegNetX backbone for slightly higher accuracy.\n\nmodel = PetModel(\"UNET\", \"resnet34\", 1e-3, num_classes, loss=\"CrossEntropy\")\n\nDownloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n\n\n\n\n\nWe’ll log metrics to TensorBoard using the TensorBoardLogger, and save the best model, measured using F1Score, with the ModelCheckpoint. Note that we use the F1Score instead of Accuracy because of the mild class imbalance.\n\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nname = \"oxfordpet\" + \"_\" + model.hparams.arch + \"_\" + model.hparams.encoder_name + \"_\" + model.hparams.loss\nlogger = TensorBoardLogger(save_dir=\"lightning_logs\",\n                           name=name,\n                           log_graph=True,\n                           default_hp_metric=False)\ncallbacks = [ModelCheckpoint(monitor=\"val_f1\",save_top_k=1, mode=\"max\") ]\n\n\nfrom itertools import islice\n\ndef show_predictions_from_batch(model, dataloader, batch_num=0, limit = None):\n    \"\"\"\n        Method to visualize model predictions from batch batch_num.\n        \n        Show a maximum of limit images.\n    \"\"\"\n    batch = next(islice(iter(dataloader), batch_num, None), None) # Selects the nth item from dataloader, returning None if not possible.\n    images, masks = batch\n\n    with torch.no_grad():\n        model.eval()\n\n        logits = model(images)\n\n    pr_masks = torch.argmax(logits,dim=1)\n\n    for i, (image, gt_mask, pr_mask) in enumerate(zip(images, masks, pr_masks)):\n        if limit and i == limit:\n            break\n        fig = plt.figure(figsize=(15,4))\n\n        ax = fig.add_subplot(1,3,1)\n        ax.imshow(image.squeeze().permute([1,2,0]))\n        ax.set_title(\"Image\")\n        ax.axis(\"off\")\n\n        ax = fig.add_subplot(1,3,2)\n        ax.imshow(gt_mask.squeeze())\n        ax.set_title(\"Ground truth\")\n        ax.axis(\"off\")\n\n        ax = fig.add_subplot(1,3,3)\n        ax.imshow(pr_mask.squeeze())\n        ax.set_title(\"Predicted mask\")\n        ax.axis(\"off\")\n\nSanity check the model by showing its predictions.\n\nshow_predictions_from_batch(model, val_dataloader, batch_num=3, limit=1)\n\n\n\n\n\n\n\n\nVisualize training progress in TesnorBoard.\n\n%load_ext tensorboard\n%tensorboard --logdir=./lightning_logs --bind_all\n\n\n\n\n\ntrainer = Trainer(accelerator='gpu',\n                  devices=1,\n                  max_epochs=12,\n                  logger=logger,\n                  callbacks=callbacks,\n                  fast_dev_run=False)\n\nINFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [&lt;class 'pytorch_lightning.callbacks.model_summary.ModelSummary'&gt;]. Skipping setting a default `ModelSummary` callback.\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n\n\nFinally, fit the model on the training dataset while saving the best model based on performance on the validation dataset.\n\ntrainer.fit(model, \n            train_dataloaders=train_dataloader,\n            val_dataloaders=val_dataloader)\n\nWARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: lightning_logs/oxfordpet_UNET_resnet34_CrossEntropy\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name          | Type             | Params | In sizes          | Out sizes        \n-------------------------------------------------------------------------------------------\n0 | model         | Unet             | 24.4 M | [16, 3, 224, 224] | [16, 3, 224, 224]\n1 | loss          | CrossEntropyLoss | 0      | ?                 | ?                \n2 | train_metrics | MetricCollection | 0      | ?                 | ?                \n3 | val_metrics   | MetricCollection | 0      | ?                 | ?                \n4 | test_metrics  | MetricCollection | 0      | ?                 | ?                \n-------------------------------------------------------------------------------------------\n24.4 M    Trainable params\n0         Non-trainable params\n24.4 M    Total params\n97.747    Total estimated model params size (MB)\n/usr/local/lib/python3.7/dist-packages/segmentation_models_pytorch/base/model.py:16: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  if h % output_stride != 0 or w % output_stride != 0:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=12` reached.\n\n\nVisualize the model performance on the validation set.\n\nshow_predictions_from_batch(model, val_dataloader, batch_num=3, limit=5)"
  },
  {
    "objectID": "posts/2022-08-25-pet_segmentation.html#analyze-best-saved-model-on-the-validation-dataset",
    "href": "posts/2022-08-25-pet_segmentation.html#analyze-best-saved-model-on-the-validation-dataset",
    "title": "Pet Segmentation",
    "section": "Analyze best saved model on the Validation dataset",
    "text": "Analyze best saved model on the Validation dataset\n\nbest_model_path = trainer.checkpoint_callback.best_model_path\nprint(best_model_path)\n\nlightning_logs/oxfordpet_UNET_resnet34_CrossEntropy/version_0/checkpoints/epoch=10-step=2530.ckpt\n\n\n\nbest_model = PetModel.load_from_checkpoint(checkpoint_path=best_model_path)\n\n\ntrainer.test(best_model,dataloaders=val_dataloader)\n\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\n────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n       Test metric             DataLoader 0\n────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n        test_acc            0.8975881338119507\n         test_f1             0.896770715713501\n        test_loss           0.27673694491386414\n────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\n[{'test_acc': 0.8975881338119507,\n  'test_f1': 0.896770715713501,\n  'test_loss': 0.27673694491386414}]\n\n\n\nshow_predictions_from_batch(best_model, val_dataloader, batch_num=3, limit=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%load_ext watermark\n%watermark --iversions -v\n\nPython implementation: CPython\nPython version       : 3.9.7\nIPython version      : 7.31.0\n\nsegmentation_models_pytorch: 0.3.0\nmatplotlib                 : 3.5.1\ntorchvision                : 0.13.0\ntorch                      : 1.12.0\nnumpy                      : 1.19.5"
  },
  {
    "objectID": "posts/2022-08-17-pet_breed_classification.html",
    "href": "posts/2022-08-17-pet_breed_classification.html",
    "title": "Pet Breed Classification",
    "section": "",
    "text": "In this example, we’ll build an image classifier to detect the 37 different dog and cat breeds in the Oxford-IIIT Pet Dataset. This is a well studied example, so our main purpose here will be to illustrate its solution using Pytorch Lightning and Torchmetrics.\nTorchmetrics is a new library that has many metrics for classification in Pytorch.\n\nIt allows for easy computation over batches.\nRigorously tested.\nA standardized interface to increase reproduciblity.\nAnd much more…\n\nWe’ll install it below with Pytorch Lightning.\n\n!pip install pytorch-lightning\n!pip install lightning-bolts\n!pip install seaborn\n!pip install watermark\n\n\nimport os\nimport torch\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom pytorch_lightning import LightningModule, Trainer, seed_everything\nfrom pl_bolts.transforms.dataset_normalizations import imagenet_normalization\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\n\nseed_everything(7)\n\nGlobal seed set to 7\n\n\n7\n\n\nLet’s quickly load and visualize the dataset. A more thorough inspection can be done using Voxel 51.\n\n# Set download=False after download is complete.\nvis_dataset = torchvision.datasets.OxfordIIITPet(root=\"./data\",split=\"trainval\",download=True)\n\nDownloading https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz to data/oxford-iiit-pet/images.tar.gz\n\n\n\n\n\nExtracting data/oxford-iiit-pet/images.tar.gz to data/oxford-iiit-pet\nDownloading https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz to data/oxford-iiit-pet/annotations.tar.gz\n\n\n\n\n\nExtracting data/oxford-iiit-pet/annotations.tar.gz to data/oxford-iiit-pet\n\n\n\nfor i in range(5):\n    image, label = vis_dataset[random.choice(range(len(vis_dataset)))]\n    \n    fig = plt.figure()\n    plt.imshow(image)\n    plt.title(f\"Image of a {vis_dataset.classes[label]}\")\n    plt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s make transforms for the training, validation, and test set. We’ll use torchvisions RandAugment which is an automated data augmentation strategy that improves the classifier accuracy by a few percentage points.\nAlso since we’ll be using a ResNet model pretrained on ImageNet, we’ll also use ImageNet normalization.\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((256,256)),\n    transforms.CenterCrop(224),\n    transforms.RandAugment(),\n    transforms.ToTensor(),\n    imagenet_normalization()\n])\nval_transform = transforms.Compose([\n    transforms.Resize((256,256)),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    imagenet_normalization()\n])\n\nMost of the remaining steps are fairly standard for obtaining a fixed train/val/test dataset. Note that the split is fixed since we used seed_everything earlier.\n\ntrain_dataset = torchvision.datasets.OxfordIIITPet(root=\"./data\", split=\"trainval\", transform=train_transform, download=False)\ntest_dataset = torchvision.datasets.OxfordIIITPet(root=\"./data\", split=\"test\",transform=val_transform, download=False)\n\n\nclasses = train_dataset.classes\nnum_classes = len(classes)\nBATCH_SIZE = 32\n\n\nsplit_len = int(0.9*len(train_dataset))\ntrain_dataset, val_dataset = random_split(train_dataset, [split_len, len(train_dataset)-split_len], generator=torch.Generator().manual_seed(7))\n\n\nprint('Length of train dataset: ', len(train_dataset))\nprint('Length of validation dataset: ', len(val_dataset))\nprint('Length of test dataset: ', len(test_dataset))\n\nLength of train dataset:  3312\nLength of validation dataset:  368\nLength of test dataset:  3669\n\n\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\nval_dataloader = torch.utils.data.DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\ntest_dataloader = torch.utils.data.DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=os.cpu_count())\n\nLet’s take a look at the class statistics in the train dataset.\n\nclass_counts = np.array([0]*num_classes)\n\nfor batch in train_dataloader:\n    _, labels = batch\n    \n    for label in labels:\n        class_counts[label] += 1\n        \ndf_classes = pd.DataFrame({\"classes\" : classes, \"class_counts\" : class_counts})\n\nAs we’ll see the classes are balanced, which simplifies the treatment of this problem.\n\ndf_classes.plot(kind=\"barh\", x=\"classes\", y=\"class_counts\",figsize=(8,12))\nplt.show()\n\n\n\n\n\n\n\n\nNow we’ll write the Pytorch Lightning module to create and train the model.\nTo train our model, we’ll freeze pretrained models from torchvision and add a linear head for fine tuning. For illustrative purposes, we’ll use a ResNet34 model, but you can also try ResNext50, or RegNetX to obtain slightly higher accuracy. The model will be set as the model attribute in the LightningModule class.\nSince the LightningModule inherits from the Pytorch Module, we also have to set the forward method of the neural network. The backward method is automatically generated.\nThe training is relatively simple, using an Adam optimizer, which is set in the configure_optimizers method.\nThe training, validation, and test steps for a batch of data are configured via the training_step, validation_step, and test_step respectively. We’ll use a single custom method called evaluate to handle the validation and test steps. See the Lightning documentation for more details.\nFinally, we’ll use Accuracy and F1Score with torchmetrics to measure the performance of our model.\n\nSince we’re using the object oriented API, which preserves state, we have to create separate objects for the train, val and test sets.\nWe wrap Accuracy and F1Score in the MetricCollection, which will automatically share common computations between the two metrics, and simplify the code.\nFor multiclass classifiers, the average option passed to the metrics (micro or macro) affects the calculation. The choices below were chosen to be consistent with scikit-learn, and work well with a balanaced dataset. I’ll delve deeper into these nuances in following posts.\n\n\nimport torch.nn as nn\nfrom torchmetrics import MetricCollection, Accuracy, F1Score\nfrom torchvision.models import resnet34, ResNet34_Weights\nfrom torchvision.models import resnext50_32x4d, ResNeXt50_32X4D_Weights\nfrom torchvision.models import regnet_x_3_2gf, RegNet_X_3_2GF_Weights\n\nclass PetModel(LightningModule):\n    \n    def __init__(self, arch, learning_rate, num_classes):\n        super().__init__()\n        self.save_hyperparameters()\n        self.example_input_array = torch.zeros((BATCH_SIZE, 3, 224,224))\n        \n        # Setup the model.\n        self.create_model(arch)\n        \n        # Setup the losses.\n        self.loss = nn.CrossEntropyLoss()\n        \n        # Setup the metrics.\n        self.train_metrics = MetricCollection({\"train_acc\" : Accuracy(num_classes=num_classes, average=\"micro\"),\n                                               \"train_f1\" : F1Score(num_classes=num_classes, average=\"macro\")})\n        self.val_metrics = MetricCollection({\"val_acc\" : Accuracy(num_classes=num_classes, average=\"micro\"),\n                                               \"val_f1\" : F1Score(num_classes=num_classes, average=\"macro\")})\n        self.test_metrics = MetricCollection({\"test_acc\" : Accuracy(num_classes=num_classes, average=\"micro\"),\n                                               \"test_f1\" : F1Score(num_classes=num_classes, average=\"macro\")})\n        \n    def create_model(self, arch):\n      \"\"\"\n        Setup a model for fine tuning.\n      \"\"\"\n      in_dimension = 512\n      if arch == \"resnext50_32x4d\":\n        self.model = resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.DEFAULT)\n        in_dimension = 4*512\n      elif arch == \"regnet_x_3_2gf\":\n        self.model = regnet_x_3_2gf(weights=RegNet_X_3_2GF_Weights.DEFAULT)\n        in_dimension = 1008\n      else:\n        self.model = resnet34(weights=ResNet34_Weights.DEFAULT)\n\n        in_dimension = 512\n\n      for param in self.model.parameters():\n        param.requires_grad = False\n\n      self.model.fc = nn.Linear(in_dimension, num_classes)\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def training_step(self, batch, batch_idx):\n        images, labels = batch\n        \n        logits = self(images)\n        preds = torch.argmax(logits, dim=1)\n        loss = self.loss(logits, labels)\n        \n        self.train_metrics(preds, labels)\n        \n        self.log(\"train_acc\", self.train_metrics[\"train_acc\"], prog_bar=True)\n        self.log(\"train_f1\", self.train_metrics[\"train_f1\"], prog_bar=True)        \n        self.log(\"train_loss\", loss, prog_bar=True)        \n        \n        return loss\n    \n    def evaluate(self, batch, stage=None):\n        images, labels = batch\n        \n        logits = self(images)\n        preds = torch.argmax(logits, dim=1)\n        loss = nn.CrossEntropyLoss()(logits, labels)\n        \n        if stage == \"val\":\n            self.val_metrics(preds,labels)\n            \n            self.log(\"val_acc\", self.val_metrics[\"val_acc\"], prog_bar=True)\n            self.log(\"val_f1\", self.val_metrics[\"val_f1\"], prog_bar=True)        \n            self.log(\"val_loss\", loss, prog_bar=True)  \n        elif stage == \"test\":\n            self.test_metrics(preds,labels)\n            \n            self.log(\"test_acc\", self.test_metrics[\"test_acc\"], prog_bar=True)\n            self.log(\"test_f1\", self.test_metrics[\"test_f1\"], prog_bar=True)        \n            self.log(\"test_loss\", loss, prog_bar=True)  \n            \n    def validation_step(self, batch, batch_idx):\n        return self.evaluate(batch, \"val\")\n    \n    def test_step(self, batch, batch_idx):\n        return self.evaluate(batch, \"test\")\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(params=self.parameters(), lr=self.hparams.learning_rate)\n\n\nmodel = PetModel(\"resnet34\", 1e-3, num_classes)\n\nWe’ll log metrics to TensorBoard using the TensorBoardLogger, and save the best model, measured using accuracy, with the ModelCheckpoint.\n\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nname = \"oxfordpet\" + \"_\" + model.hparams.arch\nlogger = TensorBoardLogger(save_dir=\"lightning_logs\",\n                           name=name,\n                           log_graph=True,\n                           default_hp_metric=False)\ncallbacks = [ModelCheckpoint(monitor=\"val_acc\",save_top_k=1, mode=\"max\") ]\n\nInitialize the trainer by requiring training to be done on the GPU, max epochs of 12, setting the logger, and callbacks.\n\ntrainer = Trainer(accelerator='gpu', \n                  devices=1,\n                  max_epochs=12,\n                  logger=logger,\n                  callbacks=callbacks,\n                  fast_dev_run=False)\n\nINFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [&lt;class 'pytorch_lightning.callbacks.model_summary.ModelSummary'&gt;]. Skipping setting a default `ModelSummary` callback.\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n\n\nVisualize training progress in TesnorBoard.\n\n%load_ext tensorboard\n%tensorboard --logdir=./lightning_logs --bind_all\n\n\n\n\nFinally, fit the model on the training dataset while saving the best model based on performance on the validation dataset.\n\ntrainer.fit(model, \n            train_dataloaders=train_dataloader,\n            val_dataloaders=val_dataloader)\n\nWARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: lightning_logs/oxfordpet_resnet34\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name          | Type             | Params | In sizes          | Out sizes\n-----------------------------------------------------------------------------------\n0 | model         | ResNet           | 21.3 M | [32, 3, 224, 224] | [32, 37] \n1 | loss          | CrossEntropyLoss | 0      | ?                 | ?        \n2 | train_metrics | MetricCollection | 0      | ?                 | ?        \n3 | val_metrics   | MetricCollection | 0      | ?                 | ?        \n4 | test_metrics  | MetricCollection | 0      | ?                 | ?        \n-----------------------------------------------------------------------------------\n19.0 K    Trainable params\n21.3 M    Non-trainable params\n21.3 M    Total params\n85.215    Total estimated model params size (MB)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=12` reached.\n\n\nLet’s load the best model and measure its performance.\n\nbest_model_path = trainer.checkpoint_callback.best_model_path\nprint(best_model_path)\n\nlightning_logs/oxfordpet_resnet34/version_0/checkpoints/epoch=7-step=832.ckpt\n\n\n\nbest_model = PetModel.load_from_checkpoint(checkpoint_path=best_model_path)\n\n\ntrainer.test(best_model,dataloaders=test_dataloader)\n\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\n────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n       Test metric             DataLoader 0\n────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n        test_acc            0.8396739363670349\n         test_f1            0.8321808576583862\n        test_loss           0.4552876353263855\n────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\n[{'test_acc': 0.8396739363670349,\n  'test_f1': 0.8321808576583862,\n  'test_loss': 0.4552876353263855}]\n\n\nNow visualize the confusion matrix.\n\nfrom torchmetrics import ConfusionMatrix\n\nconfmat = ConfusionMatrix(num_classes=num_classes, normalize='true')\nprev_device = best_model.device.type\ndevice = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n\nconfmat.to(device)\nbest_model.to(device)\nwith torch.no_grad():\n    best_model.eval()\n    \n    for batch in test_dataloader:\n        images, labels = batch\n        images, labels = images.to(device), labels.to(device)\n\n        logits = best_model(images)\n        preds = torch.argmax(logits, dim=1)\n\n        confmat.update(preds, labels)\n    \nbest_model.to(prev_device)\n1\n\n1\n\n\n\nimport seaborn as sns\ncmat = confmat.compute().cpu().numpy()\n\nfig, ax = plt.subplots(figsize=(20,20))\n\ndf_cm = pd.DataFrame(cmat, index = range(len(classes)), columns=range(len(classes)))\nax = sns.heatmap(df_cm, annot=True, fmt='.2g', cmap='Spectral')\nax.set_yticklabels([classes[i] for i in range(len(classes))], rotation=0)\nax.set_xticklabels([classes[i] for i in range(len(classes))], rotation=90)\nax.set_xlabel(\"Predicted label\")\nax.set_ylabel(\"Actual label\")\nplt.show(block=False)\n\n\n\n\n\n\n\n\n\n%load_ext watermark\n%watermark --iversions -v\n\nPython implementation: CPython\nPython version       : 3.9.7\nIPython version      : 7.31.0\n\ntorchvision: 0.13.0\nmatplotlib : 3.5.1\ntorch      : 1.12.0\npandas     : 1.3.5\nnumpy      : 1.19.5"
  },
  {
    "objectID": "posts/2022-06-15-anaconda-in-docker-on-paperspace.html",
    "href": "posts/2022-06-15-anaconda-in-docker-on-paperspace.html",
    "title": "How to build a Docker image with an Anaconda environment in Paperspace?",
    "section": "",
    "text": "Paperspace is an affordable tool for obtaining cloud compute power. Using it, you can run jupyter notebooks, python scripts, or just about anything, provided you can write the right Docker file. Plus some of the compute instances also come with GPUs, making Paperspace an alternative to Google Colab. In contrast with Google Colab though, Paperspace saves the files you use, so they are there when you return. Inspired by Alex’s post on Building my own image to use IceVision with Paperspace, in this post I’ll describe how you can build a docker image to wrap an Anaconda environment, and then run it on Paperspace. The Dockerfile and Anaconda environment I used are available on Github here.\nThe context for this post is that even though a custom anaconda environment can be created and run in the Paperspace images that are readily available, for longer projects, it would be useful to use a Paperspace image that comes prebuilt with a desired environment.\nWe can do this by adding a few additional lines to the standard Paperspace Docker image. Any anconda environment available at ‘environment.yml’ can be created and activated using these 4 lines:\nADD environment.yml $APP_HOME/environment.yml\nRUN conda env create -f $APP_HOME/environment.yml\n\nENV PATH $CONDA_DIR/envs/tf2/bin:$PATH\nRUN echo \"source activate tf2\" &gt; ~/.bashrc\nThe first two commands add the conda environment file to the Docker containers workspace, and then create the conda environment.\nThe third command adds the enviornment directory to the PATH enviroment variable. The directory path contains the environment name, which in this case is tf2 (replace this with your environment name if necessary). Finally, the last command adds a command to activate the enviornment to the bashrc file, so that the environment is activated when the container is started.\nThat’s it! The bulk of the work for creating and running a Docker image with an anaconda environment in Paperspace is done with the above 4 lines of code.\nThe entire Dockerfile is available on GitHub here.\nFollowing Alex, I’ll describe the steps to setup a Paperspace instance using this Dockerfile.\nFirst go to your notebooks inside a project, and create a new Paperspace notebook.\n\n\n\ntext\n\n\nNext, choose a GPU instance to create the notebook (or CPU depending on the project), for example the RTX5000 option, and an automatic shutdown of 6 hours.\n\n\n\ntext\n\n\nNext the Dockerfile will have to be built and pushed to Dockerhub, before it can be used with paperspace.\nTo build the Dockerfile, just run\ndocker build -t k2rajara/paperspace:3.0 .\nfor example, in the same directory as the Dockerfile.\nIn the above code, ‘k2rajara/paperspace’ is the container name on Dockerhub. To create your own repository on Dockerhub, follow the instructions here. Then push your image there. Once this is complete, its details can be added to the advanced section:\n\n\n\ntext\n\n\nThe command that is run in the container is the default one for Paperspace:\njupyter lab --allow-root --ip=0.0.0.0 --no-browser --ServerApp.trust_xheaders=True --ServerApp.disable_check_xsrf=False --ServerApp.allow_remote_access=True --ServerApp.allow_origin='*' --ServerApp.allow_credentials=True\nFinally after clicking “Start Notebook”, Paperspace should pull the image from Dockerhub and run jupyter lab as requested.\nIn my case, the conda environment I created is available on Github here and shown here:\nname: tf2\nchannels:\n  - conda-forge\n  - defaults\n  - anaconda\ndependencies:\n- python=3.6\n- pip=21\n- jupyterlab\n- jupyter_client=7.1.0\n- jupyter_server=1.13.1\n- tensorflow-gpu=2.2\n- pip:\n  - mlflow==1.11.0\n  - python-dotenv==0.15.0\n  - matplotlib==3.1.2\n  - Pillow==8.0.1\n  - scikit-image==0.17.2\n  - ruamel_yaml==0.16.12\nThe project I was working on used python 3.6, and tensorflow 2.2. Due to the outated version of python, I also had to install the jupyter client and server manually, to avoid bugs. However, the environment can be customized depending on your project requirements.\nFor example, to create a minimal installation with tensorflow2 and jupyterlab, use:\nname: tf2\nchannels:\n  - anaconda\ndependencies:\n- jupyterlab\n- tensorflow-gpu"
  },
  {
    "objectID": "posts/2022-01-13-standard-probability-distributions.html",
    "href": "posts/2022-01-13-standard-probability-distributions.html",
    "title": "Standard probability distributions",
    "section": "",
    "text": "We’ll review elementary univariate probability distributions in statistics, including how the mean/variance are derived, simple relationships, and the questions they answer, with calculations done in scipy.\nThis review is similar in spirit to brand name distributions and univariate distribution relationships.\nimport scipy.special as sp\nimport scipy.stats as ss\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nThe univariate probability distributions answer questions about events occuring on a discrete time interval \\(D_n = [0, 1, \\dotsc, n ]\\) or a continuous time interval \\(I_t = [0, t]\\).\nIn the discrete case each event could be a success with probability \\(p\\) or a failure with probability \\(1-p\\), with different events being independent. Then we can ask the following questions:\nThe first question is answered by the Binomial distribution, and the second is answered by the geometric and negative binomial distributions. A concrete example of such events are coin tosses, where the outcome of each toss is independent of previous tosses, and each toss comes up as heads with probability \\(p = \\frac{1}{2}\\).\nIn the case of a continuous time interval \\(I_t\\) there are many types of events that can occur, but we’ll study the simplest type with real world applications, the events associated with a Poisson process. Then we can ask the following questions about a Poisson process:\nThe first question is answered by the Poisson distribution, and the second is answered by the exponential and Erlang (Gamma) distributions. A concrete example of events following a Poisson process are the distribution of earthquakes in a given region.\nFinally, the normal distribution will arise as the limiting distribution in the famous central limit theorem.\nMoving forward, do remember that\n\\[ \\operatorname{var}[X] = \\mathbb{E}[(X - \\mathbb{E}[X] )^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\]"
  },
  {
    "objectID": "posts/2022-01-13-standard-probability-distributions.html#footnotes",
    "href": "posts/2022-01-13-standard-probability-distributions.html#footnotes",
    "title": "Standard probability distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe strong law of large numbers refers to almost everywhere convergence of the random variables.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Krishan Rajaratnam and I’m data scientist and mathematician. I’ve always had a strong interest in mathematics, science, and technology, and I’m able to combine many of these interests in a career applying machine learning.\nI finished my PhD at the University of Toronto under Professor Israel Michael Sigal studying the mathematical theory behind the Fractional Quantum Hall Effect (thesis available here). I also did a Master’s thesis at the University of Waterloo under Professor Dong Eui Chang and Professor Raymond G. McLenaghan studying and developing the mathematical theory behind the orthogonal separation of the Hamilton-Jacobi equation on spaces of constant curvature (thesis available here).\nBesides my technical interests, I generally have a growth mindset and am interested in staying fit, with hobbies like biking, running, and weight lifting as well as photography."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Krishan Rajaratnam's blog",
    "section": "",
    "text": "Transformer Architecture\n\n\n\nllm\n\ntransformer\n\nattention\n\n\n\nGive a brief review of the transformer (decoder only) architecture from the early days to recent improvements. The focus is on algebraic aspects with no code.\n\n\n\n\n\nAug 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nModern ETF Investing\n\n\n\ninvesting\n\nETF\n\nCanada\n\nretirement\n\n\n\nA brief introduction to exchange traded fund (ETF) investing for Canadian investors.\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nContours in OpenCV\n\n\n\nopencv\n\npython\n\n\n\nA notebook containing handy tools for working with contours in OpenCV. This includes basic contour properties, such as length, area, and 3 different methods for calculating the contours orientation.\n\n\n\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMulticlass Segmentation Metrics\n\n\n\npytorch\n\nsegmentationmodels\n\ntorchmetrics\n\n\n\nMulticlass segmentation metrics with torchmetrics, highlighting the difference between micro, macro, and macro-imagewise metrics.\n\n\n\n\n\nSep 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPet Segmentation\n\n\n\npytorch\n\npytorchlightning\n\nsegmentationmodels\n\ntorchmetrics\n\ntorchvision\n\ntensorboard\n\n\n\nA workflow for image segmentation on the Oxford IIIT pet dataset using PyTorch, PyTorch Lightning, Segmentation Models PyTorch, Torchmetrics and Tensorboard. This notebook trains state of the art image segmentation models on the Oxford IIIT pet segmentation dataset, and shows how to use torchmetrics to measure their quality.\n\n\n\n\n\nAug 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMulticlass Classification Metrics\n\n\n\npytorch\n\ntorchmetrics\n\nclassification\n\n\n\nMulticlass classification metrics with torchmetrics, highlighting the difference between micro and macro metrics.\n\n\n\n\n\nAug 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPet Breed Classification\n\n\n\npytorch\n\npytorchlightning\n\ntorchmetrics\n\ntorchvision\n\ntensorboard\n\n\n\nAn end-to-end workflow for image classification on the Oxford IIIT pet dataset using PyTorch, PyTorch Lightning, Torchmetrics and Tensorboard. This notebook trains state of the art image classification models on the Oxford IIIT pet dataset, and shows how to use torchmetrics to measure their quality.\n\n\n\n\n\nAug 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSurface Defect Segmentation\n\n\n\npytorch\n\nsegmentationmodels\n\npytorchlightning\n\nmlflow\n\n\n\nAn end-to-end workflow for image segmentation on the magnetic tile defect dataset using PyTorch, PyTorch Lightning, Segmentation models, and MLflow. This notebook trains state of the art image segmentation models on a highly imbalanced dataset, containing less than 1% of the target class.\n\n\n\n\n\nJun 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to build a Docker image with an Anaconda environment in Paperspace?\n\n\n\ndocker\n\ntools\n\n\n\nI’ll describe how to build a custom docker image containing an anaconda environment which can be run on Paperspace.\n\n\n\n\n\nJun 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPrincipal component analysis\n\n\n\nstatistics\n\n\n\nAn expository article on principal componenent analysis (PCA), starting from the theory of random vectors and then gradually moving on to concrete implementation using numpy, and scikit-learn.\n\n\n\n\n\nMar 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nStandard probability distributions\n\n\n\nstatistics\n\n\n\nAn expository article on the elementary probability distributions, their properties, and common applications, with calculations done in scipy.\n\n\n\n\n\nJan 13, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-03-30-principal-component-analysis.html",
    "href": "posts/2022-03-30-principal-component-analysis.html",
    "title": "Principal component analysis",
    "section": "",
    "text": "Principal component analysis (PCA) is a well known technique for dimensionality reduction, dating back over one hundred years. Here we’ll summarize the theory and application of PCA, showing for example that it’s most naturally applicable to multivariate normal data, which is determined by its first two moments. We shall derive PCA as a canonical form for random vectors, obtained using the population covariance matrix for a population or the sample covariance matrix for a sample. We’ll also see how this decomposition differs when using the population correlation matrix or the sample correlation matrix instead. We conclude with a computation of PCA for the iris dataset, first from scratch using numpy, and then using the scikit-learn API.\nWe follow chapter 8 in (Johnson and Wichern 1982) for the theory of PCA, both in terms of populations of random variables and samples of random variables, and then apply it following chapter 8 in (Géron 2017). We also follow Professor Helwig’s notes available here.\nBy making a distinction between population and sample PCA, we can make statements about limits of the sample PCA as the sample size goes to infinity. Besides theoretical interest, these limits are useful for calculating confidence intervals, p-values, etc.\nNote hereafter that all vectors are assumed to row vectors unless specified otherwise, and we’ll use \\(\\vec{1}\\) to denote the row vector with components all equal to \\(1\\). We’ll also make the following definitions. Let \\(\\vec{X} = (X_1, ..., X_p)\\) be a random (row) vector. The vector has (population) mean \\(\\vec{\\mu} = \\mathbb{E}[\\vec{X}]\\) and (population) covariance matrix $= [( - )^T( - )] $. The (population) correlation matrix is\n\\[\nP = \\begin{bmatrix}\n    1       & \\rho_{12} & \\rho_{13} & \\dots & \\rho_{1p} \\\\\n    \\rho_{21}       & 1 & \\rho_{23} & \\dots & \\rho_{2p} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\rho_{p1}       & \\rho_{p2} & \\rho_{p3} & \\dots & 1\n\\end{bmatrix}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\rho_{jk} & = \\frac{\\sigma_{jk}}{\\sqrt{ \\sigma_{jj} \\sigma_{kk} }}, & \\sigma_{jk} & = \\Sigma_{jk}\n\\end{aligned}\n\\]\nis the Pearson correlation coefficient between variables \\(X_j\\) and \\(X_k\\)."
  },
  {
    "objectID": "posts/2022-03-30-principal-component-analysis.html#transformations-of-random-vectors",
    "href": "posts/2022-03-30-principal-component-analysis.html#transformations-of-random-vectors",
    "title": "Principal component analysis",
    "section": "Transformations of random vectors",
    "text": "Transformations of random vectors\nFor \\(\\vec{v}, \\vec{u} \\in \\mathbb{R}^p\\), let \\(Y = \\vec{X} \\vec{v}^T = \\vec{X} \\cdot \\vec{v}\\) and \\(Z = \\vec{X} \\cdot \\vec{u}\\), then one can show that\n\\[\n\\begin{aligned}\n\\mathbb{E}[Y] & = \\vec{\\mu} \\cdot \\vec{v} \\\\\n\\operatorname{cov}[Y, Z] & = \\vec{v} \\: \\Sigma \\: \\vec{u}^T\n\\end{aligned}\n\\]\nMore generally, with \\(v_1,\\dotsc,v_q \\in \\mathbb{R}^p\\) and \\(c \\in \\mathbb{R}^q\\), let \\(Y = \\vec{X} \\cdot \\vec{v_i}\\), and define the \\(q \\times p\\) matrix \\(V\\) by:\n\\[\nV = \\begin{bmatrix}\n\\vec{v}_1 \\\\\n\\vdots \\\\\n\\vec{v}_q \\\\\n\\end{bmatrix}\n\\]\nso that \\(\\vec{Y} = \\vec{X} \\cdot V^T\\), then\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\vec{Y} + c] & = \\vec{\\mu} \\cdot V^T + c \\\\\n\\operatorname{cov}[\\vec{Y} + c ] & = \\operatorname{cov}[\\vec{X} \\cdot V^T] = V \\; \\Sigma_X \\; V^T\n\\end{aligned}\n\\]\nAlso note that\n\\[ \\sum_{j=1}^q \\operatorname{Var}(Y_j)  = \\operatorname{tr}(\\operatorname{cov}[\\vec{Y}]) = \\operatorname{tr}(V \\; \\Sigma_X \\; V^T) = \\operatorname{tr}( \\Sigma_X \\; V^T V) \\]\nby the cyclic property of the trace. Hence when \\(q = p\\) and \\(V\\) is orthonormal,\n\\[ \\sum_{j=1}^p \\operatorname{Var}(Y_j)  = \\operatorname{tr}(\\operatorname{cov}[\\vec{Y}]) = \\operatorname{tr}(A \\; \\Sigma_X \\; A^T) = \\operatorname{tr}( \\Sigma_X ) = \\sum_{j=1}^p \\operatorname{Var}(X_j) \\]\nSpectral theorem for symmetric matricies\nRecall for any \\(p \\times p\\) symmetric matrix \\(\\Sigma\\), if we let \\(\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p \\geq 0\\) be the eigenvalues of \\(\\Sigma\\), then\n\\[\n    \\begin{aligned}\n        \\lambda_1 & = \\operatorname{max}_{x \\neq 0} \\frac{\\vec{x} \\: \\Sigma \\: \\vec{x}^T}{\\lVert x \\rVert^2} \\\\\n        \\lambda_p & = \\operatorname{min}_{x \\neq 0} \\frac{\\vec{x} \\: \\Sigma \\: \\vec{x}^T}{\\lVert x \\rVert^2} \\\\\n        \\frac{\\lambda_1}{\\lambda_p} & = \\kappa(\\Sigma) = \\operatorname{max}_{x , y \\neq 0} \\frac{\\vec{x} \\: \\Sigma \\: \\vec{x}^T}{\\lVert x \\rVert^2} \\frac{\\lVert x \\rVert^2}{\\vec{y} \\: \\Sigma \\: \\vec{y}^T}\n    \\end{aligned}\n\\]\nwhere \\(\\kappa(\\Sigma)\\) is the condition number of \\(\\Sigma\\), defined when \\(\\Sigma\\) is positive definite. There are also similar formulas for the other eigenvalues defined on appropriate subspaces.\nFinally, since the covariance matrix \\(\\Sigma\\) is positive semi-definite, the spectral theorem applies to it, which we shall review later. This together with the fact that any random variable with zero variance is zero implies that \\(\\Sigma\\) is positive definite iff the random variables \\(X_1, ..., X_p\\) are linearly independent.\nTwo dimensions\nHere’s a useful corollary of the above transformation law for the covariance matrix. Suppose \\(X_1\\) and \\(X_2\\) are random variables with covariance matrix \\(\\Sigma_X\\). Then the variables \\(Y_1\\) and \\(Y_2\\) defined by\n\\[\n    \\begin{aligned}\n        Y_1 & = X_1 - X_2  \\\\\n        Y_2 & = X_1 + X_2\n    \\end{aligned}\n\\]\nhave covariance\n\\[\n    \\Sigma_Y = \\operatorname{cov}[\\vec{Y}] = \\begin{bmatrix}\n    \\sigma_{11} - 2 \\sigma_{12} + \\sigma_{22}        & \\sigma_{11} - \\sigma_{22} \\\\\n    \\sigma_{11} - \\sigma_{22}        & \\sigma_{11} + 2 \\sigma_{12} + \\sigma_{22}\n\\end{bmatrix}\n\\]\nThus \\(Y_1\\) and \\(Y_2\\) are uncorrelated iff \\(X_1\\) and \\(X_2\\) have the same variance, i.e. \\(\\sigma_{11} = \\sigma_{22}\\).\nMoreover if \\(X_1\\) and \\(X_2\\) are independent, then \\(Y_1\\) and \\(Y_2\\) are in general not independent. In fact, if \\(Y_1\\) and \\(Y_2\\) are also independent, then \\(X_1\\) and \\(X_2\\) are normal random variables (see here). This fact is known as Bernstein’s theorem, and holds for more general linear combinations.\nThus a unitary transformation of iid random variables have the same covariance matrix but are not in general independent."
  },
  {
    "objectID": "posts/2022-03-30-principal-component-analysis.html#population-pca",
    "href": "posts/2022-03-30-principal-component-analysis.html#population-pca",
    "title": "Principal component analysis",
    "section": "Population PCA",
    "text": "Population PCA\nAs above let \\(\\vec{Y} = \\vec{X} \\cdot V^T\\) where \\(V\\) is a \\(pxp\\) matrix. Then the formula\n\\[ \\Sigma_Y = \\operatorname{cov}[\\vec{Y}] = V \\; \\Sigma_X \\; V^T \\]\nshows that \\(\\Sigma_Y\\) and \\(\\Sigma_X\\) are similar quadratic forms. Thus since \\(\\Sigma_X\\) is symmetric, by the spectral theorem there exists a new set of random variables \\(Y_1, \\dotsc, Y_p\\) in which \\(\\Sigma_Y\\) is diagonal. These are the principal components of the random vector \\(\\vec{X}\\), which can also be defined using notations from probability theory as follows:\n\\[\n\\begin{aligned}\n    \\text{First principal component }  = Y_1  = & \\text{ linear combination of } \\vec{X}\\vec{v}_1^T \\text{ that maximizes } \\\\\n     & \\operatorname{Var}[\\vec{X}\\vec{v}_1^T] \\text{ subject to } \\lVert \\vec{v}_1 \\rVert = 1 \\\\\n     \\text{Second principal component }  = Y_2  = & \\text{ linear combination of } \\vec{X}\\vec{v}_2^T \\text{ that maximizes } \\\\\n     & \\operatorname{Var}[\\vec{X}\\vec{v}_2^T] \\text{ subject to } \\lVert \\vec{v}_2 \\rVert = 1 \\text{ and } \\vec{v_2} \\: \\Sigma \\: \\vec{v_1}^T = \\operatorname{Cov}[\\vec{X}\\vec{v}_1^T, \\vec{X}\\vec{v}_2^T] = 0 \\\\\n     & \\vdots \\\\\n     \\text{Last principal component }  = Y_p  = & \\text{ linear combination of } \\vec{X}\\vec{v}_p^T \\text{ that maximizes } \\\\\n     & \\operatorname{Var}[\\vec{X}\\vec{v}_p^T] \\text{ subject to } \\lVert \\vec{v}_p \\rVert = 1 \\text{ and } \\operatorname{Cov}[\\vec{X}\\vec{v}_p^T, \\vec{X}\\vec{v}_1^T] = \\operatorname{Cov}[\\vec{X}\\vec{v}_p^T, \\vec{X}\\vec{v}_2^T] = \\dots = \\operatorname{Cov}[\\vec{X}\\vec{v}_p^T, \\vec{X}\\vec{v}_{p-1}^T] = 0\n\\end{aligned}\n\\]\nEquivalently, let \\((\\lambda_1, \\vec{e}_1)\\), \\((\\lambda_2, \\vec{e}_2)\\), …, \\((\\lambda_p, \\vec{e}_p)\\) be the eigenvalue-eigenvector pairs of \\(\\Sigma\\) with the eigenvalues ordered in non-increasing order. Then the ith principal component is:\n\\[\nY_i = \\vec{X}\\vec{e}_i^T \\quad, i = 1,...,p\n\\]\nwith\n\\[\n\\begin{aligned}\n\\operatorname{Var}[Y_i]  & = \\lambda_i \\quad i = 1,...,p \\\\\n\\operatorname{Cov}[Y_i, Y_j]  & = 0 \\quad i \\neq j\n\\end{aligned}\n\\]\nThus the ith maximal variance of \\(\\Sigma\\) is the ith eigenvalue of \\(\\Sigma\\).\nAlso note that only unique eigenvalues have unique eigenvectors (up to sign), so the principal components are in general not unique.\nThe above formula for the trace implies that the random variables \\(Y_1, ..., Y_p\\) have the same total variance as \\(X_1, ..., X_p\\), since\n\\[ \\sum_{j=1}^p \\operatorname{Var}(X_j)  = \\operatorname{tr}(\\Sigma) = \\operatorname{tr}(V \\; \\Sigma_X \\; V^T ) = \\operatorname{tr}(\\Sigma_X) = \\sum_{j=1}^p \\operatorname{Var}(Y_j) = \\sum_{j=1}^p \\lambda_j \\]\nThus we can define the proportion of variance captured by the \\(k\\)th principal component as\n\\[\n\\begin{aligned}\n& \\text{Proporition of variance due }  = \\frac{\\lambda_k}{\\sum_{j=1}^p \\lambda_j}, \\quad k= 1,...,p \\\\\n& \\text{to the $k$th principal component }\n\\end{aligned}\n\\]\nThe basic idea behind PCA for dimensionality reduction is to only keep the the first \\(k\\) principal components which captures, say \\(90\\%\\), of the variance, thereby reducing the number of variables.\nAnother way to select the principal components is to choose the most variables possible such that the conditon number (ratio of largest to smallest eigenvalues) of the reduced covariance matrix is close to \\(1\\). This is because machine learning algorithms, like linear and logistic regression, preform better (more precision and faster convergence for gradient based methods) when the condition number of the covariance matrix is close to \\(1\\).\n\nPopulation PCA using the correlation matrix\nPCA can also be calculated using the correlation matrix \\(P\\) which is the covariance matrix of the standardized features having their mean set to \\(0\\) and variance to \\(1\\). Specifically, one applies the above calculations to the following variables:\n\\[\n\\begin{aligned}\n    Z_1 &= \\frac{X_1 - \\mu_1}{\\sqrt{\\sigma_{11}}} \\\\\n    Z_2 &= \\frac{X_2 - \\mu_2}{\\sqrt{\\sigma_{22}}} \\\\\n    \\vdots & \\quad \\vdots \\\\\n    Z_p &= \\frac{X_p - \\mu_p}{\\sqrt{\\sigma_{pp}}}\n\\end{aligned}\n\\]\nIn matrix notation we have\n\\[\n\\begin{aligned}\n\\vec{Z} &= (\\vec{W}^{\\frac{1}{2}})^{-1}(\\vec{X} - \\vec{\\mu}) \\\\\n\\operatorname{cov}[\\vec{Z}] & = (\\vec{W}^{\\frac{1}{2}})^{-1} \\; \\Sigma \\; (\\vec{W}^{\\frac{1}{2}})^{-1} = P\n\\end{aligned}\n\\]\nwhere \\(\\vec{W} = \\operatorname{diag}(\\sigma_{11},...,\\sigma_{pp})\\). Then one obtains a decomposition similar to above with the following difference in the formula:\n\\[\n\\sum_{j=1}^p \\operatorname{Var}(Y_j) = \\sum_{j=1}^p \\operatorname{Var}(X_j) = p\n\\]\nThen the formula for the proportion of variance captured by the \\(k\\)th principal component simplifies to:\n\\[\n\\begin{aligned}\n& \\text{Proporition of variance due }  = \\frac{\\lambda_k}{p}, \\quad k= 1,...,p \\\\\n& \\text{to the $k$th principal component }\n\\end{aligned}\n\\]\nIt’s important to note that the principal components calculated from \\(\\Sigma\\) and \\(P\\) can differ significantly, being a different linear combination of the original variables (see Johnson and Wichern 1982 Example 8.2). The principal components are similar when the variances for the \\(p\\) random variables are the same, \\(\\sigma = \\sigma_{11} = \\sigma_{22} = \\dots = \\sigma_{pp}\\). Finally note that it’s good practise to work with variables on the same scales so that the principal components aren’t heavily skewed towards the larger variables."
  },
  {
    "objectID": "posts/2022-03-30-principal-component-analysis.html#sample-pca",
    "href": "posts/2022-03-30-principal-component-analysis.html#sample-pca",
    "title": "Principal component analysis",
    "section": "Sample PCA",
    "text": "Sample PCA\nConsider \\(n\\) iid realizations \\(\\vec{X}_1, ..., \\vec{X}_n\\) from the random vector \\(\\vec{X}\\). We can arrange this data into a feature matrix\n\\[\nX =  \\begin{bmatrix}\n\\vec{X}_1 \\\\\n\\vdots \\\\\n\\vec{X}_n \\\\\n\\end{bmatrix}\n\\]\nThe sample covariance matrix, \\(S\\), is:\n\\[\n\\begin{aligned}\nS_{i j}  = \\frac{1}{n-1} \\sum_{k=1}^n (X_{k i} - \\bar{X}_i) (X_{k j} - \\bar{X}_j) = \\frac{1}{n-1} (X - \\vec{1} \\otimes\\bar{X})^T (X - \\bar{X} \\otimes \\vec{1})\n\\end{aligned} \\quad \\quad i= 1,...,p \\text{ and } j = 1,...,p\\]\nand the sample correlation matrix, \\(R\\), is:\n\\[\n\\begin{aligned}\nR_{ij} = \\frac{S_{ij}}{\\sqrt{S_{ii}} \\sqrt{S_{jj}}} = \\frac{\\sum_{k=1}^n (X_{k i} - \\bar{X}_i) (X_{k j} - \\bar{X}_j)}{\\sqrt{\\sum_{k=1}^n (X_{k i} - \\bar{X}_i)^2 } \\sqrt{\\sum_{k=1}^n (X_{k j} - \\bar{X}_j)^2}}\n\\end{aligned} \\quad \\quad i= 1,...,p \\text{ and } j = 1,...,p\n\\]\nFor \\(\\vec{v} \\in \\mathbb{R}^p\\) a new feature is defined by the equation\n\\[ Y = \\vec{X} \\cdot \\vec{v} \\]\nFormulas similar to above show that the sample covariance matrix transforms according to (see chapter 8 in Johnson and Wichern 1982):\n\\[ S_Y = V S_X V^T \\]\nThus an identical construction can be given for the principal components, now using \\(S\\) in place of \\(\\Sigma\\), or \\(R\\) in place of \\(P\\).\n\nReconstruction error\nLet \\(W_d = [ v_1 \\dots v_d ]\\) be the matrix whose columns are the first \\(d\\) eigenvectors of \\(S_X\\) (ordered from largest to smallest eigenvalue). Then given any feature vector \\(\\vec{X}_i\\), the vector \\(\\vec{X}_i W_d\\) is the \\(d\\) dimensional projection, and the vector \\(\\vec{X}_i W_d W_d^T\\) is the reconstructed vector. There’s a fairly simple formula for the reconstruction error\n\\[\n\\sum_{i=1}^n || \\vec{X}_i W_d W_d^T - \\vec{X}_i||^2\n\\]\nIt turns out that (see chapter 9 in Johnson and Wichern 1982 for a proof)\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n || \\vec{X}_i W_d W_d^T - \\vec{X}_i||^2 & = \\operatorname{tr}[(X W_d W_d^T - X)(X W_d W_d^T - X)^T] \\\\\n&  \\vdots \\\\\n& = \\lambda_{d+1} + \\dots + \\lambda_p\n\\end{aligned}\n\\]\nThus the recontruction error of the \\(d\\) dimensional projection is equal to the remaining \\(p-d\\) eigenvalues of the covariance matrix, i.e. the unexplained variance. Finally, note that this is the smallest possible error among all \\(d\\) dimensional projections, and this is another defining property of PCA."
  },
  {
    "objectID": "posts/2022-03-30-principal-component-analysis.html#large-sample-properties",
    "href": "posts/2022-03-30-principal-component-analysis.html#large-sample-properties",
    "title": "Principal component analysis",
    "section": "Large sample properties",
    "text": "Large sample properties\nAs expected, when the feature matrix \\(X\\) comes from a multivariate Gaussian, the principal components recover the natural frame which diagonalizes the covariance matrix of the Gaussian. In fact, much more can be said.\nWhen \\(\\vec{X}_i \\overset{iid}{\\sim} N(\\vec{\\mu}, \\vec{\\Sigma})\\) and the eigenvalues of \\(\\Sigma\\) are strictly positive and unique: \\(\\lambda_1 &gt; \\dots \\lambda_p &gt; 0\\), we can describe the large sample properties of the principal directions and eigenvalues. Then as \\(n \\rightarrow \\infty\\), we have (Johnson and Wichern 1982):\n\\[\n    \\sqrt{n}(\\hat{\\vec{\\lambda}} - \\vec{\\lambda}) \\approx N(\\vec{0}, 2 \\vec{\\Lambda}^2) \\\\\n    \\sqrt{n}(\\hat{\\vec{v}}_k - \\vec{v}_k) \\approx N(\\vec{0}, \\vec{V}_k)\n\\]\nwhere \\(\\Lambda = \\operatorname{diag}(\\lambda_1, ..., \\lambda_k)\\) and\n\\[\n    \\vec{V}_k = \\lambda_k \\sum_{l \\neq k} \\frac{\\lambda_l}{(\\lambda_l - \\lambda_k)^2} \\vec{v}_l^T \\vec{v}_l\n\\]\nFurthermore, as \\(n \\rightarrow \\infty\\), we have that \\(\\hat{\\lambda}_k\\) and \\(\\hat{\\vec{v}}_k\\) are independent."
  },
  {
    "objectID": "posts/2022-03-30-principal-component-analysis.html#practical-considerations",
    "href": "posts/2022-03-30-principal-component-analysis.html#practical-considerations",
    "title": "Principal component analysis",
    "section": "Practical considerations",
    "text": "Practical considerations\nBelow we follow A. Geron’s notes on PCA to implement it in numpy and sckit-learn for the iris dataset.\n\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nimport numpy as np\n\niris = datasets.load_iris()\n\n\nprint(iris['feature_names'])\n\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n\n\n\ndf = pd.DataFrame(iris['data'], columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\ndf.head(5)\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\nSince the units of each feature are comparable (the standard deviations are of the same order of magnitude), we will calculate PCA using the sample covariance matrix. If on the otherhand, the magnitudes differed by orders of magnitude, we would calculate PCA using the sample correlation matrix.\n\nPCA using SVD directly\nThe principal components can be calculated by diagonalizing the covariance matrix or calulating the singular value decomposition of the feature matrix. We’ll use the latter for numerical reasons.\n\nX = iris['data']\n\nThe SVD of \\(X\\) is calculated after subtracting off the sample mean. We’ll call this matrix \\(X_{centered}\\) and work with it hereafter. Then the SVD of \\(X_{centered}\\) is\n\\(X_{centered} = U D V^T\\)\nwhere \\(U\\) is an \\(n \\times n\\) orthonormal matrix, \\(V\\) is an \\(m \\times m\\) orthonormal matrix, and \\(D = \\operatorname{diag}(d_{11}, ..., d_{mm})\\) is an \\(n \\times m\\) rectangular diagonal matrix.\nThe matrix \\(V\\) is the matrix of eigenvectors from the sample PCA above, and the diagonal covariance matrix \\(S_Y\\) is obtained by squaring the singular values and dividing by \\(n-1\\):\n\\[\nS_Y =   \\frac{1}{n-1}\\begin{bmatrix}\n    d_{11}^2 & & \\\\\n    & \\ddots & \\\\\n    & & d_{mm}^2\n  \\end{bmatrix}\n\\]\nFor this example, we have:\n\n# Direct computation using SVD\nX_centered = X - X.mean(axis=0)\nU, D, Vt = np.linalg.svd(X_centered)\nV = Vt.T\neigs = D ** 2\n\nThe \\(d\\)-dimensional projection can be calcualted using the \\(p \\times d\\) matrix \\(W_d\\) whose \\(d\\) columns are the first \\(d\\) eigenvectors.\n\\(X_{d-proj} = X_{centered}    W_d\\)\n\nW2 = V[:, :2]\n# W2 can also be obtained from the sklearn package using: pca.components_.T \nX2D_diy = X_centered @ W2\nX2D_diy.shape\n\n(150, 2)\n\n\nObtain the reconstructed data using:\n$ X_{recovered} = X_{d-proj} W_{d}^T$\nA defining property of PCA is that for any dimension \\(d &lt; m\\), the PCA reconstruction of the data minimizes the mean squared error between the original data among all possible \\(d\\) dimensional hyperplanes. Moreover, the square of this error is equal to the sum of the \\(m-d\\) smaller eigenvalues, as we shall see.\n\nX_recovered = X2D_diy @ W2.T\n\nThe squared error is then:\n\nnp.linalg.norm(X_centered - X_recovered)**2\n\n15.204644359438948\n\n\nWhich is equal to the sum of the two smaller eigenvalues:\n\nsum(eigs[2:])\n\n15.204644359438953\n\n\nThe proportion of the total sample variance due to each sample principal component is:\n\\[\n\\frac{\\hat{\\lambda_i}}{(\\sum_{i=1}^m \\hat{\\lambda_i}) } \\quad \\quad \\quad i = 1,...,m\n\\]\nand is given numerically by\n\neigs/sum(eigs)\n\narray([0.92461872, 0.05306648, 0.01710261, 0.00521218])\n\n\nWe can obtain the number of principal components to retain by either (i) retaining the first \\(d\\) components which sum up to 95% of the total variance, or (ii) looking for an elbow in the scree plot.\n\nimport matplotlib.pyplot as plt\n\nplt.plot(range(1,len(eigs)+1), np.array(eigs.tolist()))\nplt.title(label=\"Scree plot\")\nplt.xlabel(\"i\")\nplt.ylabel(\"lambda_i\")\nplt.show()\n\n\n\n\n\n\n\n\nThere is an elbow in the plot at \\(i = 2\\), meaning all eigenvalues after \\(\\lambda_1\\) are relatively small and about the same size. In this case, it appears that the first principal component effectively summarize the total sample variance.\nFinally, we can visualize the two dimensional projection using a scatter plot.\n\nplt.title(\"PCA with 2 principal components\", fontsize=14)\nplt.scatter(X2D_diy[:, 0], X2D_diy[:,1], c=iris['target'])\nplt.xlabel(\"$y_1$\", fontsize=18)\nplt.ylabel(\"$y_2$\", fontsize=18)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPCA using scikit-learn\nNow we’ll calculate the PCA projection using scikit-learn. Notice that scikit-learn will mean center the data itself.\n\npca = PCA(n_components = 2)\nX2D = pca.fit_transform(X)\n\nInstead of specifing the number of components in the call to PCA above, we can also set \\(\\operatorname{n\\_components}=0.95\\) to obtain the number of dimensions which capture 95% of the variance in the data. Or we could pass in no arguments to get the full SVD and plot a scree plot.\nBelow, we check that sklearn PCA and the one calculated with SVD directly give the same answer.\n\nnp.max(X2D - X2D_diy)\n\n2.748330173586095\n\n\nThe non-uniquness of the PCA leads to a large difference. However, this can be fixed by inspecting the projections and adjusting:\n\nX2D_diy[:,1] = - X2D_diy[:,1]\nnp.max(X2D - X2D_diy)\n\n1.3322676295501878e-15\n\n\nThe explained variance ratio holds the proportion of variance explained by the first two sample principal components, and is given by\n\npca.explained_variance_ratio_\n\narray([0.92461872, 0.05306648])\n\n\n\nComputational complexity\nLet \\(n_{max} = \\operatorname{max}(m,n)\\) and \\(n_{min} = \\operatorname{min}(m,n)\\). If \\(n_{max} &lt; 500\\) and \\(d\\) is less than \\(80\\%\\) of \\(n_{min}\\), then scikit-learn uses the full SVD approach which has a complexity of \\(O(n_{max}^2 n_{min})\\), otherwise it uses randomized PCA which has a complexity of \\(O(n_{max}^2 d)\\) (cf. here and (Géron 2017)).\nThere are many more variations of PCA, and other dimensionality reduction algorithms. See Decomposing signals in components (matrix factorization problems) in the scikit-learn documentation, for example."
  },
  {
    "objectID": "posts/2022-06-30-surface-defect-segmentation.html",
    "href": "posts/2022-06-30-surface-defect-segmentation.html",
    "title": "Surface Defect Segmentation",
    "section": "",
    "text": "In this notebook we use the segmentation models library built in PyTorch to train image segmentation models for the Magnetic tile defect dataset. This dataset is interesting because it is highly imbalanced, with less than 1% of pixels corresponding to the target class. Using the segmentation models library we can try several different loss functions, including binary cross entropy, focal losss, and Tversky loss to see their performance (see here for recommended loss functions). However, it turns out that using binary cross entropy is sufficient to get good results.\nAditionally, following an example in the segmentation models library, we’ll use PyTorch Lightning to further simplify the training process in PyTorch and MLflow to log hyperparameters and metrics.\nThis code is built with the help of Detection of Surface Defects in Magnetic Tile Images by Dr. Mitra P. Danesh.\nTo install the required packages, usually it’s sufficient to run the cell below (for example on Google Colab or Paperspace using the PyTorch runtime). If PyTorch is not installed, then uncomment the first line of the cell below to install it.\n# Uncomment the following line if run in the jupyter tensorflow Docker image.\n!pip install torch torchvision \n!pip install -U git+https://github.com/qubvel/segmentation_models.pytorch\n!pip install pytorch-lightning\n!pip install mlflow\n!pip install watermark\nimport os\nimport math\nimport random\n\nimport torch\nimport numpy as np\nimport segmentation_models_pytorch as smp\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset as BaseDataset\n\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import MLFlowLogger\n\n# set the random seeds for reproducibility\nrandom.seed(42)\ntorch.manual_seed(0)\nnp.random.seed(0)"
  },
  {
    "objectID": "posts/2022-06-30-surface-defect-segmentation.html#loading-data",
    "href": "posts/2022-06-30-surface-defect-segmentation.html#loading-data",
    "title": "Surface Defect Segmentation",
    "section": "Loading data",
    "text": "Loading data\n\nfrom torch.utils.data import random_split\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom glob import glob\n\nFirst download the data locally.\n\n%%capture\n!wget -O data.zip https://github.com/abin24/Magnetic-tile-defect-datasets./archive/master.zip\n!unzip data.zip\n!mv Magnetic-tile-defect-datasets.-master data\n\n\nclasses =['Blowhole', 'Crack','Free'] # classes/labels\nimage_paths = []\n\nfor c in classes:\n    # retreive image file paths recursively\n    images_found = glob('data/MT_' + c + '/Imgs/*.jpg',recursive=True) \n    if c== 'Free': # undersample the free class.\n        image_paths.extend( images_found[:80] )\n    else:\n        image_paths.extend( images_found )        \n    \nrandom.shuffle(image_paths)\n\n\nlen(image_paths)\n\n252\n\n\n\nDataset\nWriting helper class for data extraction, tranformation and preprocessing\nhttps://pytorch.org/docs/stable/data. Also see the binary segmentation intro in the segmentation models library for more details on designing the Dataset class. For more sophisticated data augmentations, see the albumentations library and specifically this notebook from segmentation models.\n\nimport torchvision.transforms.functional as TF\nimport random\n\nclass SurfaceDefectDetectionDataset(Dataset):\n    def __init__(self, image_path_list, use_transform=False):\n        super().__init__()\n        self.image_path_list = image_path_list\n        self.use_transform = use_transform\n        \n    def transform(self, image, target):\n        if random.random() &lt; 0.5:\n            image = TF.hflip(image)\n            target = TF.hflip(target)\n    \n        if random.random() &lt; 0.5:\n            image = TF.vflip(image)\n            target = TF.vflip(target)\n    \n        angle = random.choice([0, -90, 90, 180])\n        image, target = TF.rotate(image, angle), TF.rotate(target, angle)\n    \n        return image, target\n        \n    def __len__(self):\n        return len(self.image_path_list)\n    \n    def __getitem__(self, idx):\n        # Open the image file which is in jpg  \n        image = Image.open(self.image_path_list[idx])\n        # The mask is in png. \n        # Use the image path, and change its extension to png to get the mask's path.\n        mask = Image.open(os.path.splitext(self.image_path_list[idx])[0]+'.png') \n        \n        # resize the images.\n        image, mask = TF.resize(image, (320,320)), TF.resize(mask, (320,320))\n        \n        # Perform augmentation if required.\n        if self.use_transform:\n            image, mask = self.transform(image, mask)\n        \n        # Transform the image and mask PILs to torch tensors. \n        image, mask = TF.to_tensor(image), TF.to_tensor(mask)\n        \n        # Threshold mask, threshold limit is 0.5\n        mask = (mask &gt;= 0.5)*(1.0)\n        \n        #return the image and mask pair tensors\n        return image, mask\n\n\nsplit_len = int(0.8*len(image_paths))\ntrain_dataset = SurfaceDefectDetectionDataset(image_paths[:split_len], use_transform = True)\ntest_dataset = SurfaceDefectDetectionDataset(image_paths[split_len:], use_transform = False)\n\nWe’ll randomly split the train and validation set, but fix the random seed to fix these datasets.\n\ntrain_dataset, val_dataset = random_split(train_dataset, [int(split_len*0.9), split_len - int(split_len*0.9)], generator=torch.Generator().manual_seed(1))\n\n\nprint('Length of train dataset: ', len(train_dataset))\nprint('Length of validation dataset: ', len(val_dataset))\nprint('Length of test dataset: ', len(test_dataset))\n\nLength of train dataset:  180\nLength of validation dataset:  21\nLength of test dataset:  51\n\n\n\nLet’s take a look at the dataset\n\nimport matplotlib.pyplot as plt\nimport random\n\nsample_img, sample_msk = train_dataset[random.choice(range(len(train_dataset)))]\nplt.subplot(1,2,1)\nplt.title(\"Sample from trainining set\")\nplt.axis(\"off\")\nplt.imshow(sample_img.squeeze(), cmap='gray')\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.imshow(sample_msk.squeeze(), cmap='gray')\nplt.show()\n\nsample_img, sample_msk = val_dataset[random.choice(range(len(val_dataset)))]\nplt.subplot(1,2,1)\nplt.title(\"Sample from validation set\")\nplt.axis(\"off\")\nplt.imshow(sample_img.squeeze(), cmap='gray')\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.imshow(sample_msk.squeeze(), cmap='gray')\nplt.show()\n\nsample_img, sample_msk = test_dataset[random.choice(range(len(test_dataset)))]\nplt.subplot(1,2,1)\nplt.title(\"Sample from test set\")\nplt.axis(\"off\")\nplt.imshow(sample_img.squeeze(), cmap='gray')\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.imshow(sample_msk.squeeze(), cmap='gray')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake a look at more samples from the train set.\n\nfor i in range(3):\n    sample_img, sample_msk = train_dataset[random.choice(range(len(train_dataset)))]\n    plt.subplot(1,2,1)\n    plt.title(\"Image\")\n    plt.axis(\"off\")\n    plt.imshow(sample_img.squeeze(), cmap='gray')\n    plt.subplot(1,2,2)\n    plt.title(\"Mask\")\n    plt.axis(\"off\")\n    plt.imshow(sample_msk.squeeze(), cmap='gray')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind the weight of positive and negative pixels.\nThe number of positive pixels is less than 1% of the total, showing that the dataset is highly imbalanced.\n\npositive_weight = 0\nnegative_weight = 0\ntotal_pixels = 0\nimg_shape = train_dataset[0][0].shape\nfor _, target in train_dataset:\n    positive_weight += (target &gt;= 0.5).sum().item()\n    negative_weight += (target &lt; 0.5).sum().item()\n    total_pixels += (img_shape[1] * img_shape[2])\npositive_weight /= total_pixels\nnegative_weight /= total_pixels\nprint('positive weight = ',positive_weight, '\\tnegative weight = ', negative_weight)\n\npositive weight =  0.002119411892361111     negative weight =  0.9978805881076389"
  },
  {
    "objectID": "posts/2022-06-30-surface-defect-segmentation.html#create-model-and-train",
    "href": "posts/2022-06-30-surface-defect-segmentation.html#create-model-and-train",
    "title": "Surface Defect Segmentation",
    "section": "Create model and train",
    "text": "Create model and train\n\nfrom itertools import islice\n\ndef show_predictions_from_batch(model, dataloader, batch_num=0, limit = None):\n    \"\"\"\n        Method to visualize model predictions from batch batch_num.\n        \n        Show a maximum of limit images.\n    \"\"\"\n    batch = next(islice(iter(dataloader), batch_num, None), None) # Selects the nth item from dataloader, returning None if not possible.\n    images, masks = batch\n\n    with torch.no_grad():\n        model.eval()\n\n        logits = model(images)\n\n    pr_masks = logits.sigmoid()\n    pr_masks = (pr_masks &gt;= 0.5)*1\n\n    for i, (image, gt_mask, pr_mask) in enumerate(zip(images, masks, pr_masks)):\n        if limit and i == limit:\n            break\n        fig = plt.figure(figsize=(15,4))\n\n        ax = fig.add_subplot(1,3,1)\n        ax.imshow(image.squeeze(), cmap='gray')\n        ax.set_title(\"Image\")\n        ax.axis(\"off\")\n\n        ax = fig.add_subplot(1,3,2)\n        ax.imshow(gt_mask.squeeze(), cmap='gray')\n        ax.set_title(\"Ground truth\")\n        ax.axis(\"off\")\n\n        ax = fig.add_subplot(1,3,3)\n        ax.imshow(pr_mask.squeeze(), cmap='gray')\n        ax.set_title(\"Predicted mask\")\n        ax.axis(\"off\")\n\nWe’ll create a PyTorch Lightning module to help streamline the training process. In the class initalization, it uses the segmentation models library, via the call to smp.create_model, to build a PyTorch model which operates on one channel images for binary segmentation. Many state of the art models are possible with the segmentation models library. However, we’ll typically use the Unet with resnet34 backbone.\nWe use the SoftBCEWithLogitsLoss to train the model, but other losses recommended here, can easily be used as well.\nWe’ll also use the segmentation models library to monitor the intersection over union metric. Due to the highly imbalanced nature of this dataset, this will give us a much better indicator of model quality than the accuracy.\n\nclass SurfaceDefectModel(pl.LightningModule):\n\n    def __init__(self, arch, encoder_name, loss = \"SoftBCEWithLogitsLoss\" , **kwargs):\n        super().__init__()\n        self.model = smp.create_model(\n            arch, encoder_name=encoder_name, encoder_weights = None, in_channels=1, classes=1, **kwargs\n        )\n\n        self.arch = arch\n        self.encoder_name = encoder_name\n        \n        self.loss_name = loss\n        if loss == \"DiceLoss\":\n            self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n        elif loss == \"TverskyLoss\":\n            self.loss_fn = smp.losses.TverskyLoss(smp.losses.BINARY_MODE, from_logits=True, alpha=0.3,beta=0.7)\n        elif loss == \"FocalLoss\":\n            self.loss_fn = smp.losses.FocalLoss(smp.losses.BINARY_MODE)              \n        else:\n            self.loss_fn = smp.losses.SoftBCEWithLogitsLoss()\n            \n        self.printed_run_id = None\n        self.run_id = None\n        \n    def forward(self, image):\n        return self.model(image)\n\n    def shared_step(self, batch, stage):\n        \n        image = batch[0]\n\n        # Shape of the image should be (batch_size, num_channels, height, width)\n        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n        assert image.ndim == 4\n\n        # Check that image dimensions are divisible by 32, \n        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of \n        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have \n        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -&gt; 5, 10, 20, 40, 80\n        # and we will get an error trying to concat these features\n        h, w = image.shape[2:]\n        assert h % 32 == 0 and w % 32 == 0\n\n        mask = batch[1]\n\n        # Shape of the mask should be [batch_size, num_classes, height, width]\n        # for binary segmentation num_classes = 1\n        assert mask.ndim == 4\n\n        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n        assert mask.max() &lt;= 1.0 and mask.min() &gt;= 0\n\n        logits_mask = self.forward(image)\n        \n        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n        loss = self.loss_fn(logits_mask, mask)\n\n        # Lets compute metrics for some threshold\n        # first convert mask values to probabilities, then \n        # apply thresholding\n        prob_mask = logits_mask.sigmoid()\n        pred_mask = (prob_mask &gt; 0.5).float()\n\n        # We will compute IoU metric by two ways\n        #   1. dataset-wise\n        #   2. image-wise\n        # but for now we just compute true positive, false positive, false negative and\n        # true negative 'pixels' for each image and class\n        # these values will be aggregated in the end of an epoch\n        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"binary\")\n\n        return {\n            \"loss\": loss,\n            \"tp\": tp,\n            \"fp\": fp,\n            \"fn\": fn,\n            \"tn\": tn,\n        }\n\n    def shared_epoch_end(self, outputs, stage):\n        # aggregate step metics\n        tp = torch.cat([x[\"tp\"] for x in outputs])\n        fp = torch.cat([x[\"fp\"] for x in outputs])\n        fn = torch.cat([x[\"fn\"] for x in outputs])\n        tn = torch.cat([x[\"tn\"] for x in outputs])\n        \n        # per image IoU means that we first calculate IoU score for each image \n        # and then compute mean over these scores\n        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n        \n        # dataset IoU means that we aggregate intersection and union over whole dataset\n        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n        # in this particular case will not be much, however for dataset \n        # with \"empty\" images (images without target class) a large gap could be observed. \n        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n\n        accuracy = smp.metrics.accuracy(tp, fp, fn, tn)\n        \n        metrics = {\n            f\"{stage}_per_image_iou\": per_image_iou,\n            f\"{stage}_dataset_iou\": dataset_iou,\n            f\"{stage}_accuracy\": accuracy,\n            f\"{stage}_loss\": torch.tensor([x[\"loss\"].item() for x in outputs]).mean()\n        }\n        \n        # Log the metrics\n        #for key, val in metrics.items():\n        #    self.logger.experiment.log_metric(self.logger.run_id ,key,  val.mean().item(), step=self.current_epoch)\n        self.logger.log_metrics({key: val.mean().item() for key, val in metrics.items() }, step=self.current_epoch)\n        \n        # only record the loss in mlflow\n        del metrics[f\"{stage}_loss\"]\n        if not self.printed_run_id and hasattr(self.logger, \"run_id\"):\n            print('Run id: ', self.logger.run_id )\n            self.printed_run_id = True\n            \n        # This will be available in tensorboard.\n        self.log_dict(metrics, prog_bar=True)\n\n    def training_step(self, batch, batch_idx):\n        return self.shared_step(batch, \"train\")            \n\n    def training_epoch_end(self, outputs):\n        self.shared_epoch_end(outputs, \"train\")\n\n    def validation_step(self, batch, batch_idx):\n        return self.shared_step(batch, \"valid\")\n\n    def validation_epoch_end(self, outputs):\n        self.shared_epoch_end(outputs, \"valid\")\n\n    def test_step(self, batch, batch_idx):\n        return self.shared_step(batch, \"test\")  \n\n    def test_epoch_end(self, outputs):\n        return self.shared_epoch_end(outputs, \"test\")\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.0001)\n    \n    def on_fit_end(self):\n        # Log hyperparameters to mlflow.\n        self.logger.log_hyperparams({ \"arch\": self.arch, \"encoder_name\": self.encoder_name, \"loss\": self.loss_name })\n        if hasattr(self.logger, \"run_id\"):\n          self.run_id = self.logger.run_id\n\nNote that we added the extra hook on_fit_end to save hyperparameters to MLflow. More hooks are available at the official documentation lightning hooks.\n\nbatch_size = 8\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nvalid_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n\nmodel = SurfaceDefectModel(\"Unet\", \"resnet34\")\n\nSanity check the model by showing its predictions.\n\nshow_predictions_from_batch(model, train_loader, batch_num=0, limit=1)\n\n\n\n\n\n\n\n\nWe’ll use the ModelCheckpoint callback from PyTorch lightning to save the best model, as measured by the intersection over union metric.\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pathlib import Path\n\ncheckpoint_callback = ModelCheckpoint(\n    monitor=\"valid_dataset_iou\",\n    dirpath=\"./models\",\n    filename= f\"surface_defect_{model.arch}_{model.encoder_name}_{model.loss_name}\",\n    save_top_k=3,\n    mode=\"max\",\n)\n\n# Add the model directory if it it doesn't exist\nPath(\"./models\").mkdir(exist_ok=True)\n\nNow with the help of PyTorch lightning, we can train and log to MLflow, with a few lines of code.\n\nmlf_logger = MLFlowLogger(experiment_name=\"lightning_logs\")\ntrainer = pl.Trainer(\n    gpus=1, \n    max_epochs=200,\n    log_every_n_steps=math.ceil(len(train_dataset)/batch_size),\n    callbacks=[checkpoint_callback],\n    logger=mlf_logger,\n    # For debugging purposes, uncomment the line below.\n    #fast_dev_run=True \n)\n\ntrainer.fit(\n    model, \n    train_dataloaders=train_loader, \n    val_dataloaders=valid_loader,\n)\n\nLoad the best model to analyze its performance.\n\nmodel.load_from_checkpoint(f\"models/surface_defect_{model.arch}_{model.encoder_name}_{model.loss_name}.ckpt\", arch = model.arch, encoder_name= model.encoder_name, loss = model.loss_fn.__class__.__name__)\n\n\n# run validation dataset\ntrainer.validate(model, dataloaders=valid_loader, verbose=False)\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\n[{'valid_accuracy': 0.9991360902786255,\n  'valid_dataset_iou': 0.6411741971969604,\n  'valid_per_image_iou': 0.7908229827880859}]\n\n\nVisualize the model performance on the validation set.\n\nfor i in range(min(len(valid_loader), 5)):\n    show_predictions_from_batch(model, valid_loader, batch_num=i)"
  },
  {
    "objectID": "posts/2022-06-30-surface-defect-segmentation.html#analyze-best-saved-model-on-the-test-set",
    "href": "posts/2022-06-30-surface-defect-segmentation.html#analyze-best-saved-model-on-the-test-set",
    "title": "Surface Defect Segmentation",
    "section": "Analyze best saved model on the Test set",
    "text": "Analyze best saved model on the Test set\n\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n\n\n# run test dataset\ntrainer.test(model, dataloaders=test_loader, verbose=False)\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\n[{'test_accuracy': 0.9993589520454407,\n  'test_dataset_iou': 0.6736205816268921,\n  'test_per_image_iou': 0.7249590158462524}]\n\n\nFinally, visualize the model performance on the test set.\n\nshow_predictions_from_batch(model, test_loader, batch_num=0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%load_ext watermark\n%watermark --iversions -v\n\nPython implementation: CPython\nPython version       : 3.9.7\nIPython version      : 7.31.0\n\ntorch                      : 1.12.0\npytorch_lightning          : 1.6.4\nnumpy                      : 1.19.5\ntorchvision                : 0.13.0\nPIL                        : 8.4.0\nsegmentation_models_pytorch: 0.3.0"
  },
  {
    "objectID": "posts/2022-08-18-multiclass_classification_metrics.html",
    "href": "posts/2022-08-18-multiclass_classification_metrics.html",
    "title": "Multiclass Classification Metrics",
    "section": "",
    "text": "In my last post I showed how to use torchmetrics to implement classification metrics for the Oxford-IIIT pet dataset. We saw that the average keyword had to be set to micro for accuracy and macro for the \\(F1\\) score, so that the metrics were consistent with scikit-learn. In this post, I’ll delve deeper into these keywords, and how they affect the metrics in question. In a following post, I’ll also discuss mdmc_average, which is relevant for multiclass image segmentation.\nNote that this keyword is relevant for binary classifiers, which are also mutliclass clasffiers with \\(2\\) classes.\nThe examples below will look primarily at accuracy and precision, but note that precision can be replaced by recall, \\(F1\\) score, jaccard index, etc.\n\n!pip install pytorch-lightning\n!pip install seaborn\n\n\nimport torch\nimport sklearn \n\n# Set the seed for reproduciblity.\ntorch.manual_seed(7)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nTo better understand the metrics, we’ll work with a \\(4\\) class problem with \\(n = 100\\) samples. Classes \\(0\\) and \\(3\\) will have a probability of occurence of \\(\\frac{1}{15}\\), class \\(1\\) will have a probability of \\(\\frac{2}{3}\\), and class \\(2\\) will have a probability of \\(\\frac{1}{5}\\). We can generate data having this distribution using torch.multinomial below.\n\nweights = torch.tensor([1, 10, 3, 1], dtype=torch.float)\nnum_classes = len(weights)\nshape = (100,)\nsize = shape[0]\noutput = torch.multinomial(weights, size, replacement=True)\ntarget = torch.multinomial(weights, size, replacement=True)\n\nFor example, the output looks like:\n\noutput\n\ntensor([1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 0, 0, 1, 0, 1, 2, 0, 3, 1, 3, 2, 2, 1, 1,\n        2, 1, 3, 2, 1, 3, 1, 1, 1, 1, 1, 0, 1, 0, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1,\n        1, 2, 0, 1, 1, 2, 1, 2, 3, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 2, 3, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n        2, 2, 1, 1])\n\n\nFor our purposes, it’ll be more convienent to work with one hot encoded data.\n\noutput_oh = torch.zeros(size, num_classes)\noutput_oh[torch.arange(size), output] = 1\ntarget_oh = torch.zeros(size, num_classes)\ntarget_oh[torch.arange(size), target] = 1\n\nThen the first \\(10\\) samples of the output looks like:\n\noutput_oh[:10]\n\ntensor([[0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 1., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 1., 0.]])\n\n\n\nMicro statistics\nAfter the one hot encoding, the output and target tensors each have shape \\([N, C]\\). Due to this multidimensionality, there are at least two ways to compute the confusion matrix. For the first way, we calculate the confusion matrix of each sample over all classes:\n\nfrom torchmetrics.classification import StatScores\n\n# Use reduce=\"samples\" to calculate the confusion matrix per sample.\nstat_scores = StatScores(num_classes=num_classes, reduce=\"samples\")\nstats = stat_scores(output_oh, target)\ntp, fp, tn, fn, _ = stats[:, 0], stats[:, 1], stats[:, 2], stats[:, 3], stats[:, 4]\n\nFor example for the first \\(3\\) samples, the confusion matricies are:\n\nax = sns.heatmap(torch.vstack([tp[:3], fp[:3], tn[:3], fn[:3]]).T.numpy(),annot=True,\n                 annot_kws={\"fontsize\":12},linewidths=2, cbar=False,cmap=ListedColormap(['white']))\nax.set_xlabel(\"Stats\", fontsize=16)\nax.set_ylabel(\"Sample\", fontsize=16)\nax.set_title(\"Micro averaging\", fontsize=16)\nax.set_xticklabels([\"tp\", \"fp\", \"tn\", \"fn\"], fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\nThe confusion matrix for the entire classifier is then obtained by summing over the samples:\n\nax = sns.heatmap(torch.tensor([[tp.sum(), fn.sum()], [fp.sum(), tn.sum()]]).numpy(),annot=True,\n                 annot_kws={\"fontsize\":12},linewidths=2, cbar=False,cmap=ListedColormap(['white']))\nax.set_xlabel(\"Predicted\", fontsize=16)\nax.set_ylabel(\"Actual\", fontsize=16)\nax.set_yticklabels([\"T\", \"F\"], fontsize=14)\nax.set_xticklabels([\"P\", \"N\"], fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\nWe could’ve also reached the above answer using:\n\nstat_scores = StatScores(num_classes=num_classes, reduce=\"micro\")\nstats = stat_scores(output_oh, target)\nstats[:4]\n\ntensor([ 40,  60, 240,  60])\n\n\nFrom which various metrics like accuracy, precision, and recall can be calculated. When these metrics are calculated this way, the averaging technique is called micro.  For example, the accuracy, i.e. the number of correctly classified samples divided by the total samples, is\n\n((tp.sum())/(tp.sum()+fn.sum())).item()\n\n0.4000000059604645\n\n\nSimilarly, using torchmetrics we get\n\nfrom torchmetrics.functional.classification import accuracy\n\naccuracy(output_oh, target,num_classes=num_classes,average=\"micro\").item()\n\n0.4000000059604645\n\n\nWhich is precisely what scikit-learn calculates:\n\nsklearn.metrics.accuracy_score(target, output)\n\n0.4\n\n\nSince the false positive and false negative counts are always the same, all other metrics like precision and recall  are the same as accuracy. For example the precision is\n\n((tp.sum())/(tp.sum()+fp.sum())).item()\n\n0.4000000059604645\n\n\nSimilarly, using torchmetrics we have:\n\nfrom torchmetrics.functional.classification import precision\n\nprecision(output_oh, target,num_classes=num_classes,average=\"micro\").item()\n\n0.4000000059604645\n\n\nThis is why micro statistics are rarely mentioned, because they don’t give rise to new metrics.\n\n\nMacro statistics\nThe second way to calculate the confusion matrix is to calculate the statistics for each class separately over all samples.\n\nstat_scores = StatScores(num_classes=num_classes, reduce=\"macro\")\nstats = stat_scores(output_oh, target)\ntp, fp, tn, fn, _ = stats[:,0], stats[:, 1], stats[:, 2], stats[:, 3], stats[:, 4]\n\nwhich gives the confusion matricies for classes \\(0\\), \\(1\\),…,\\(3\\).\n\nax = sns.heatmap(torch.vstack([tp, fp, tn, fn]).T.numpy(),annot=True,\n                 annot_kws={\"fontsize\":12},linewidths=2, cbar=False,cmap=ListedColormap(['white']))\nax.set_xlabel(\"Stats\", fontsize=16)\nax.set_ylabel(\"Class\", fontsize=16)\nax.set_title(\"Macro averaging\", fontsize=16)\nax.set_xticklabels([\"tp\", \"fp\", \"tn\", \"fn\"], fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\nFrom which various metrics like precision, and recall can be calculated by calculating the metric for each class and then averaging. When these metrics are calculated this way, the averaging technique is called macro.  For example, the precision, is\n\n(tp/(tp + fp)).mean().item()\n\n0.20424403250217438\n\n\nSimilarly, using torchmetrics we have:\n\nfrom torchmetrics.functional.classification import precision\n\nprecision(output,target,num_classes=num_classes,average=\"macro\").item()\n\n0.20424403250217438\n\n\nwhich is the same as scikit-learn:\n\n# Note that scikit-learn takes the target as the first argument instead\n# of the second.\nsklearn.metrics.precision_score(target, output,average=\"macro\")\n\n0.2042440318302387\n\n\nFinally, it’s important to note that class \\(1\\) is much more probable than the others, hence its precision score should be weighted differently than the others. In other words, when working with an imbalanced dataset like this one, it makes sense to use a weighted average:\n\nprecision(output,target,num_classes=num_classes,average=\"weighted\").item()\n\n0.43180373311042786\n\n\nWhich clearly paints a different picture of the quality of the predictions.\nIn conclusion when dealing with balanaced datasets, accuracy using the micro average is sufficient, while the \\(F1\\) score with the weighted average is more accurate for imbalanaced datasets.\n\n\nReferences\n\nTorchmetrics Quickstart\nMulticlass and multilabel classification in scikit-learn"
  },
  {
    "objectID": "posts/2022-09-06-multiclass_segmentation_metrics.html",
    "href": "posts/2022-09-06-multiclass_segmentation_metrics.html",
    "title": "Multiclass Segmentation Metrics",
    "section": "",
    "text": "In my last post I showed how to use torchmetrics to implement segmentation metrics for the Oxford-IIIT pet segmentation dataset. We saw that in addition to the average keyword introduced in the pet breed classification post, the mdmc_average keyword is necessary to compute metrics for image data.\nIn this post we’ll dive deeper into these metrics, explaining the two choices for the mdmc_average parameter, including global and samplewise, as well as giving recommendations for dealing with imbalanced datasets.\nThe examples below will look primarily at precision and \\(F1\\) score, but note that these metrics can be replaced by recall, dice score, etc.\n!pip install pytorch-lightning\n!pip install -U git+https://github.com/qubvel/segmentation_models.pytorch\n!pip install seaborn\n!pip install watermark\nimport torch\nimport functools\nimport segmentation_models_pytorch as smp\nfrom torchmetrics.functional.classification import precision, f1_score\nfrom torchmetrics.classification import StatScores\nfrom sklearn import metrics\n\n# Set the seed for reproduciblity.\ntorch.manual_seed(7)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nTo better understand the metrics, we’ll work with a \\(4\\) class problem with \\(n = 100\\) samples. Classes \\(0\\) and \\(3\\) will have a probability of occurence of \\(\\frac{1}{15}\\), class \\(1\\) will have a probability of \\(\\frac{2}{3}\\), and class \\(2\\) will have a probability of \\(\\frac{1}{5}\\). We can generate data having this distribution using torch.multinomial below.\nweights = torch.tensor([1, 10, 3, 1], dtype=torch.float)\nnum_classes = len(weights)\nshape = (100,1,256,256)\nsize = functools.reduce(lambda x, y : x* y, shape)\noutput = torch.multinomial(weights, size, replacement=True).reshape(shape)\noutput[70:,:,:,:] = torch.zeros(30, *shape[1:])\ntarget = torch.multinomial(weights, size, replacement=True).reshape(shape)\ntarget[70:,:,:,:] = torch.zeros(30, *shape[1:])\nFor example, a subset of the output looks like:\noutput[0,:,:10,:10]\n\ntensor([[[1, 1, 2, 1, 2, 2, 1, 1, 2, 2],\n         [1, 1, 1, 1, 1, 3, 1, 1, 1, 1],\n         [1, 1, 3, 2, 1, 2, 1, 1, 1, 1],\n         [0, 1, 1, 2, 3, 1, 1, 1, 1, 2],\n         [1, 0, 1, 1, 1, 1, 1, 1, 1, 3],\n         [1, 1, 1, 2, 0, 1, 1, 0, 1, 1],\n         [1, 1, 1, 0, 1, 1, 2, 1, 2, 1],\n         [2, 1, 1, 1, 2, 1, 2, 1, 3, 2],\n         [3, 1, 1, 3, 1, 2, 1, 1, 1, 1],\n         [2, 3, 0, 1, 1, 1, 1, 2, 2, 1]]])\nFirst we can collapse the image dimensions, \\(H\\) and \\(W\\), and then calculate metrics as for multiclass classification. This is precisely what happens when we choose mdmc_average global.\nprecision(output, target,num_classes=num_classes,average=\"macro\",mdmc_average=\"global\").item()\n\n0.4517214596271515\nFor comparisons sake, in scikit-learn we have:\nmetrics.precision_score(target.reshape((-1)).numpy(),output.reshape((-1)).numpy(), average=\"macro\")\n\n0.4517214441963613\nThen the different options for average can be chosen, including micro, macro, and weighted.\nIn contrast, the image dimensions can be treated separately, which is called the macro-imagewise reduction:\nThis is the most natural way to calculate metrics like the Jaccard index (intersection over union) for example. Unfortunately the jaccard index can’t be calculated this way using torchmetrics. However the \\(F1\\)/Dice Score can be calculated using torchmetrics, and it’s equivalent to the Jaccard index1:\nf1_score(output, target,num_classes=num_classes,average=\"macro\",mdmc_average=\"samplewise\").item()\n\n0.2497853934764862\nHowever if we calculate the \\(F1\\) score using the segmentation models library, we get:\ntp, fp, fn, tn = smp.metrics.get_stats(output.long(), target.long(), mode='multiclass', num_classes=num_classes)\nsmp.metrics.f1_score(tp, fp, fn, tn, reduction=\"macro-imagewise\").item()\n\n0.47478538751602173\nThis is because our dataset has many images with no targets (recall that we zeroed out several images). Thus the \\(F1\\) score for non-background classes reduces to \\(\\frac{0}{0}\\). smp replaces occurences of \\(\\frac{0}{0}\\) by \\(1\\), while torchmetrics replaces \\(\\frac{0}{0}\\) by \\(0\\). If we pass zero_division=0 to the segmentation models library, we get the same value as torchmetrics:\ntp, fp, fn, tn = smp.metrics.get_stats(output.long(), target.long(), mode='multiclass', num_classes=num_classes)\nsmp.metrics.f1_score(tp, fp, fn, tn, reduction=\"macro-imagewise\", zero_division=0).item()\n\n0.2497853934764862\nThis we why we recommend avoiding mdmc_average equal to samplewise, and calculating the metrics like for regular multiclass classifiers instead.\nIn conclusion when dealing with balanaced datasets, accuracy using the micro average plus mdmc_average global is sufficient, while the \\(F1\\) score with the weighted average plus mdmc_average global is more accurate for imbalanaced datasets.\n%load_ext watermark\n%watermark --iversions\n\nseaborn                    : 0.11.2\nmatplotlib                 : 3.5.1\nsegmentation_models_pytorch: 0.3.0\ntorch                      : 1.12.0\nsklearn                    : 1.0.2"
  },
  {
    "objectID": "posts/2022-09-06-multiclass_segmentation_metrics.html#footnotes",
    "href": "posts/2022-09-06-multiclass_segmentation_metrics.html#footnotes",
    "title": "Multiclass Segmentation Metrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‘This fact is discussed further here’↩︎"
  },
  {
    "objectID": "posts/2024-05-20-modern-etf-investing.html",
    "href": "posts/2024-05-20-modern-etf-investing.html",
    "title": "Modern ETF Investing",
    "section": "",
    "text": "s&p500-returns\n\n\nSources: The Growth of $100 by Asset Class (1970-2023) and Historical Returns on Stocks, Bonds and Bills: 1928-2023\nAccording to the above figure, over the 53 year period from 1970 to 2023, the S&P 500 grew an average of 10.75% per year, whereas cash grew an average of 4.35% per year. The cash growth just beat inflation, which was 3.98% per year during that period. Here “Cash” is measured in the value of 3-month U.S. treasury bills (T-bills). It’s well known that leaving money in actual cash loses value over time, because prices of common items like bread, and soap and tend to increase over time.\nWhy is this useful to the average person? At an annual growth rate of 10.75%, an initial saving of $1000 plus monthly investments of $400 per month can grow to $1 million dollars in 30 years. You can do the calculation yourself here.\nThe traditional approach to get good returns, similar to 10.75%, is to invest in mutual funds, which invest your money in a variety of financial products, including stocks and bonds. They often claim to give great returns, even better than the S&P 500 (which is unlikely to hold for 20-30 years (see Hallam 2017 for example)). However they come at a large cost, measured by the management expense ratio (MER). The MER is a percentage that is deducted from the annual returns of the underlying fund. This MER calculator comapres an initial investment of $10000 with monthly contributions of $1000 over 25 years with investments that have the same return but significantly different MERs. The first investment has an MER of 0.24%, comparable to many Vanguard funds, and the second has an MER of 2.23%, the MER of expensive mutual funds. The difference in fees over 25 years is over $225 thousand dollars.\nThe solution for most people is to inveset in index funds. Index funds are designed to passively track an index like the S&P 500 US stock market index, S&P/TSX Composite Index, bond market indices, etc. Since they are passively managed, they typically have a much lower MER than mutual funds, which are actively managed. Most brokerages sell exchange traded funds (ETFs), which are index funds that can be traded like stocks.\nWith thousands of ETFs to choose from these days, there are still challenges. The solution is to pick ETFs with low fees (MER &lt; 0.5%), a diversified portfolio, and be consistient over the long run (quoted from canadiancouchpotato.com). Vanguard ETFs are a great choice for having well diversified ETFs with low fees, which I go with. There’s also BlackRock (iShares) and BMO in Canada. Simple examples of balanced (containing stocks and bonds) ETF portfolios are available here. For example, VBAL is an ETF by Vangaurd Canada that has been around since 2018 and has a 5 year growth rate of 6.30%, with an MER of 0.24%, thus an effective 5 year growth rate of 6.06%. This ETF is an “end-user” ETF that tracks an internal benchmark, but claims to “invest in broad-based equity and fixed income markets”, so it’s safe to assume it’s diversified. See the top 10 holdings for the specific ETFs it aggregates. This ETF gives a fairly easy option for a low fee, diversified portfolio.\nFinally, a plan to practically implement ETF investing by automatically investing deposits, and reinvesting dividends can be found at (Bortolotti 2024) here. See (Hallam 2017) for a much more detailed walkthrough on ETF investing. (Lespérance 2024) for a more detailed guide on ETF investing online for Canadians. (Sethi 2019) for another introduction to ETF investing and much more about personal finance.\n\n\n\n\nReferences\n\nBortolotti, Dan. 2024. “Canadian Couch Potato.” 2024. https://canadiancouchpotato.com/.\n\n\nHallam, Andrew. 2017. Millionaire Teacher: The Nine Rules of Wealth You Should Have Learned in School 2nd Edition. Hoboken, NJ: John Wiley & Sons.\n\n\nLespérance, Jean. 2024. “How to Invest Online: Guide to Self-Directed Investing.” 2024. http://howtoinvestonline.blogspot.com/2014/05/howtoinvestonline-guide-to-self.html.\n\n\nSethi, Ramit. 2019. I Will Teach You to Be Rich 2nd Edition. New York, NY: Workman Publishing."
  }
]