<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-03-30">
<meta name="description" content="An expository article on principal componenent analysis (PCA), starting from the theory of random vectors and then gradually moving on to concrete implementation using numpy, and scikit-learn.">

<title>Principal component analysis – Krishan Rajaratnam’s blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.svg" rel="icon" type="image/svg+xml">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-dfb324f25d9b1687192fa8be62ac8f9c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Krishan Rajaratnam’s blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/krishanr"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/KrishanRajarat1"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Principal component analysis</h1>
                  <div>
        <div class="description">
          An expository article on principal componenent analysis (PCA), starting from the theory of random vectors and then gradually moving on to concrete implementation using numpy, and scikit-learn.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">statistics</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 30, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#transformations-of-random-vectors" id="toc-transformations-of-random-vectors" class="nav-link active" data-scroll-target="#transformations-of-random-vectors">Transformations of random vectors</a></li>
  <li><a href="#population-pca" id="toc-population-pca" class="nav-link" data-scroll-target="#population-pca">Population PCA</a>
  <ul class="collapse">
  <li><a href="#population-pca-using-the-correlation-matrix" id="toc-population-pca-using-the-correlation-matrix" class="nav-link" data-scroll-target="#population-pca-using-the-correlation-matrix">Population PCA using the correlation matrix</a></li>
  </ul></li>
  <li><a href="#sample-pca" id="toc-sample-pca" class="nav-link" data-scroll-target="#sample-pca">Sample PCA</a></li>
  <li><a href="#large-sample-properties" id="toc-large-sample-properties" class="nav-link" data-scroll-target="#large-sample-properties">Large sample properties</a></li>
  <li><a href="#practical-considerations" id="toc-practical-considerations" class="nav-link" data-scroll-target="#practical-considerations">Practical considerations</a>
  <ul class="collapse">
  <li><a href="#pca-using-svd-directly" id="toc-pca-using-svd-directly" class="nav-link" data-scroll-target="#pca-using-svd-directly">PCA using SVD directly</a></li>
  <li><a href="#pca-using-scikit-learn" id="toc-pca-using-scikit-learn" class="nav-link" data-scroll-target="#pca-using-scikit-learn">PCA using scikit-learn</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="2022-03-30-principal-component-analysis.out.ipynb" download="2022-03-30-principal-component-analysis.out.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Principal component analysis (PCA) is a well known technique for dimensionality reduction, dating back over one hundred years. Here we’ll summarize the theory and application of PCA, showing for example that it’s most naturally applicable to multivariate normal data, which is determined by its first two moments. We shall derive PCA as a canonical form for random vectors, obtained using the population covariance matrix for a population or the sample covariance matrix for a sample. We’ll also see how this decomposition differs when using the population correlation matrix or the sample correlation matrix instead. We conclude with a computation of PCA for the iris dataset, first from scratch using numpy, and then using the scikit-learn API.</p>
<p>We follow chapter 8 in <span class="citation" data-cites="Johnson1982AppliedMS">(<a href="#ref-Johnson1982AppliedMS" role="doc-biblioref">Johnson and Wichern 1982</a>)</span> for the theory of PCA, both in terms of populations of random variables and samples of random variables, and then apply it following chapter 8 in <span class="citation" data-cites="Gron2017HandsOnML">(<a href="#ref-Gron2017HandsOnML" role="doc-biblioref">Géron 2017</a>)</span>. We also follow Professor Helwig’s notes available <a href="http://users.stat.umn.edu/~helwig/notes/pca-Notes.pdf">here</a>.</p>
<p>By making a distinction between population and sample PCA, we can make statements about limits of the sample PCA as the sample size goes to infinity. Besides theoretical interest, these limits are useful for calculating confidence intervals, p-values, etc.</p>
<p>Note hereafter that all vectors are assumed to row vectors unless specified otherwise, and we’ll use <span class="math inline">\(\vec{1}\)</span> to denote the row vector with components all equal to <span class="math inline">\(1\)</span>. We’ll also make the following definitions. Let <span class="math inline">\(\vec{X} = (X_1, ..., X_p)\)</span> be a random (row) vector. The vector has (population) mean <span class="math inline">\(\vec{\mu} = \mathbb{E}[\vec{X}]\)</span> and (population) covariance matrix $= [( - )^T( - )] $. The (population) correlation matrix is</p>
<p><span class="math display">\[
P = \begin{bmatrix}
    1       &amp; \rho_{12} &amp; \rho_{13} &amp; \dots &amp; \rho_{1p} \\
    \rho_{21}       &amp; 1 &amp; \rho_{23} &amp; \dots &amp; \rho_{2p} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    \rho_{p1}       &amp; \rho_{p2} &amp; \rho_{p3} &amp; \dots &amp; 1
\end{bmatrix}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
\rho_{jk} &amp; = \frac{\sigma_{jk}}{\sqrt{ \sigma_{jj} \sigma_{kk} }}, &amp; \sigma_{jk} &amp; = \Sigma_{jk}
\end{aligned}
\]</span></p>
<p>is the Pearson correlation coefficient between variables <span class="math inline">\(X_j\)</span> and <span class="math inline">\(X_k\)</span>.</p>
<section id="transformations-of-random-vectors" class="level2">
<h2 class="anchored" data-anchor-id="transformations-of-random-vectors">Transformations of random vectors</h2>
<p>For <span class="math inline">\(\vec{v}, \vec{u} \in \mathbb{R}^p\)</span>, let <span class="math inline">\(Y = \vec{X} \vec{v}^T = \vec{X} \cdot \vec{v}\)</span> and <span class="math inline">\(Z = \vec{X} \cdot \vec{u}\)</span>, then one can show that</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[Y] &amp; = \vec{\mu} \cdot \vec{v} \\
\operatorname{cov}[Y, Z] &amp; = \vec{v} \: \Sigma \: \vec{u}^T
\end{aligned}
\]</span></p>
<p>More generally, with <span class="math inline">\(v_1,\dotsc,v_q \in \mathbb{R}^p\)</span> and <span class="math inline">\(c \in \mathbb{R}^q\)</span>, let <span class="math inline">\(Y = \vec{X} \cdot \vec{v_i}\)</span>, and define the <span class="math inline">\(q \times p\)</span> matrix <span class="math inline">\(V\)</span> by:</p>
<p><span class="math display">\[
V = \begin{bmatrix}
\vec{v}_1 \\
\vdots \\
\vec{v}_q \\
\end{bmatrix}
\]</span></p>
<p>so that <span class="math inline">\(\vec{Y} = \vec{X} \cdot V^T\)</span>, then</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[\vec{Y} + c] &amp; = \vec{\mu} \cdot V^T + c \\
\operatorname{cov}[\vec{Y} + c ] &amp; = \operatorname{cov}[\vec{X} \cdot V^T] = V \; \Sigma_X \; V^T
\end{aligned}
\]</span></p>
<p>Also note that</p>
<p><span class="math display">\[ \sum_{j=1}^q \operatorname{Var}(Y_j)  = \operatorname{tr}(\operatorname{cov}[\vec{Y}]) = \operatorname{tr}(V \; \Sigma_X \; V^T) = \operatorname{tr}( \Sigma_X \; V^T V) \]</span></p>
<p>by the cyclic property of the trace. Hence when <span class="math inline">\(q = p\)</span> and <span class="math inline">\(V\)</span> is orthonormal,</p>
<p><span class="math display">\[ \sum_{j=1}^p \operatorname{Var}(Y_j)  = \operatorname{tr}(\operatorname{cov}[\vec{Y}]) = \operatorname{tr}(A \; \Sigma_X \; A^T) = \operatorname{tr}( \Sigma_X ) = \sum_{j=1}^p \operatorname{Var}(X_j) \]</span></p>
<p><strong>Spectral theorem for symmetric matricies</strong></p>
<p>Recall for any <span class="math inline">\(p \times p\)</span> symmetric matrix <span class="math inline">\(\Sigma\)</span>, if we let <span class="math inline">\(\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_p \geq 0\)</span> be the eigenvalues of <span class="math inline">\(\Sigma\)</span>, then</p>
<p><span class="math display">\[
    \begin{aligned}
        \lambda_1 &amp; = \operatorname{max}_{x \neq 0} \frac{\vec{x} \: \Sigma \: \vec{x}^T}{\lVert x \rVert^2} \\
        \lambda_p &amp; = \operatorname{min}_{x \neq 0} \frac{\vec{x} \: \Sigma \: \vec{x}^T}{\lVert x \rVert^2} \\
        \frac{\lambda_1}{\lambda_p} &amp; = \kappa(\Sigma) = \operatorname{max}_{x , y \neq 0} \frac{\vec{x} \: \Sigma \: \vec{x}^T}{\lVert x \rVert^2} \frac{\lVert x \rVert^2}{\vec{y} \: \Sigma \: \vec{y}^T}
    \end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\kappa(\Sigma)\)</span> is the condition number of <span class="math inline">\(\Sigma\)</span>, defined when <span class="math inline">\(\Sigma\)</span> is positive definite. There are also similar formulas for the other eigenvalues defined on appropriate subspaces.</p>
<p>Finally, since the covariance matrix <span class="math inline">\(\Sigma\)</span> is positive semi-definite, the spectral theorem applies to it, which we shall review later. This together with the fact that any random variable with zero variance is zero implies that <span class="math inline">\(\Sigma\)</span> is positive definite iff the random variables <span class="math inline">\(X_1, ..., X_p\)</span> are linearly independent.</p>
<p><strong>Two dimensions</strong></p>
<p>Here’s a useful corollary of the above transformation law for the covariance matrix. Suppose <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are random variables with covariance matrix <span class="math inline">\(\Sigma_X\)</span>. Then the variables <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> defined by</p>
<p><span class="math display">\[
    \begin{aligned}
        Y_1 &amp; = X_1 - X_2  \\
        Y_2 &amp; = X_1 + X_2
    \end{aligned}
\]</span></p>
<p>have covariance</p>
<p><span class="math display">\[
    \Sigma_Y = \operatorname{cov}[\vec{Y}] = \begin{bmatrix}
    \sigma_{11} - 2 \sigma_{12} + \sigma_{22}        &amp; \sigma_{11} - \sigma_{22} \\
    \sigma_{11} - \sigma_{22}        &amp; \sigma_{11} + 2 \sigma_{12} + \sigma_{22}
\end{bmatrix}
\]</span></p>
<p>Thus <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are uncorrelated iff <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> have the same variance, i.e.&nbsp;<span class="math inline">\(\sigma_{11} = \sigma_{22}\)</span>.</p>
<p>Moreover if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent, then <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are in general not independent. In fact, if <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are also independent, then <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are normal random variables (see <a href="https://math.stackexchange.com/questions/42115/when-linear-combinations-of-independent-random-variables-are-still-independent">here</a>). This fact is known as <em>Bernstein’s theorem</em>, and holds for more general linear combinations.</p>
<p>Thus a unitary transformation of iid random variables have the same covariance matrix but are not in general independent.</p>
</section>
<section id="population-pca" class="level2">
<h2 class="anchored" data-anchor-id="population-pca">Population PCA</h2>
<p>As above let <span class="math inline">\(\vec{Y} = \vec{X} \cdot V^T\)</span> where <span class="math inline">\(V\)</span> is a <span class="math inline">\(pxp\)</span> matrix. Then the formula</p>
<p><span class="math display">\[ \Sigma_Y = \operatorname{cov}[\vec{Y}] = V \; \Sigma_X \; V^T \]</span></p>
<p>shows that <span class="math inline">\(\Sigma_Y\)</span> and <span class="math inline">\(\Sigma_X\)</span> are similar quadratic forms. Thus since <span class="math inline">\(\Sigma_X\)</span> is symmetric, by the spectral theorem there exists a new set of random variables <span class="math inline">\(Y_1, \dotsc, Y_p\)</span> in which <span class="math inline">\(\Sigma_Y\)</span> is diagonal. These are the <em>principal components</em> of the random vector <span class="math inline">\(\vec{X}\)</span>, which can also be defined using notations from probability theory as follows:</p>
<p><span class="math display">\[
\begin{aligned}
    \text{First principal component }  = Y_1  = &amp; \text{ linear combination of } \vec{X}\vec{v}_1^T \text{ that maximizes } \\
     &amp; \operatorname{Var}[\vec{X}\vec{v}_1^T] \text{ subject to } \lVert \vec{v}_1 \rVert = 1 \\
     \text{Second principal component }  = Y_2  = &amp; \text{ linear combination of } \vec{X}\vec{v}_2^T \text{ that maximizes } \\
     &amp; \operatorname{Var}[\vec{X}\vec{v}_2^T] \text{ subject to } \lVert \vec{v}_2 \rVert = 1 \text{ and } \vec{v_2} \: \Sigma \: \vec{v_1}^T = \operatorname{Cov}[\vec{X}\vec{v}_1^T, \vec{X}\vec{v}_2^T] = 0 \\
     &amp; \vdots \\
     \text{Last principal component }  = Y_p  = &amp; \text{ linear combination of } \vec{X}\vec{v}_p^T \text{ that maximizes } \\
     &amp; \operatorname{Var}[\vec{X}\vec{v}_p^T] \text{ subject to } \lVert \vec{v}_p \rVert = 1 \text{ and } \operatorname{Cov}[\vec{X}\vec{v}_p^T, \vec{X}\vec{v}_1^T] = \operatorname{Cov}[\vec{X}\vec{v}_p^T, \vec{X}\vec{v}_2^T] = \dots = \operatorname{Cov}[\vec{X}\vec{v}_p^T, \vec{X}\vec{v}_{p-1}^T] = 0
\end{aligned}
\]</span></p>
<p>Equivalently, let <span class="math inline">\((\lambda_1, \vec{e}_1)\)</span>, <span class="math inline">\((\lambda_2, \vec{e}_2)\)</span>, …, <span class="math inline">\((\lambda_p, \vec{e}_p)\)</span> be the eigenvalue-eigenvector pairs of <span class="math inline">\(\Sigma\)</span> with the eigenvalues ordered in non-increasing order. Then the ith principal component is:</p>
<p><span class="math display">\[
Y_i = \vec{X}\vec{e}_i^T \quad, i = 1,...,p
\]</span></p>
<p>with</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{Var}[Y_i]  &amp; = \lambda_i \quad i = 1,...,p \\
\operatorname{Cov}[Y_i, Y_j]  &amp; = 0 \quad i \neq j
\end{aligned}
\]</span></p>
<p>Thus the ith maximal variance of <span class="math inline">\(\Sigma\)</span> is the ith eigenvalue of <span class="math inline">\(\Sigma\)</span>.</p>
<p>Also note that only unique eigenvalues have unique eigenvectors (up to sign), so the principal components are in general not unique.</p>
<p>The above formula for the trace implies that the random variables <span class="math inline">\(Y_1, ..., Y_p\)</span> have the same total variance as <span class="math inline">\(X_1, ..., X_p\)</span>, since</p>
<p><span class="math display">\[ \sum_{j=1}^p \operatorname{Var}(X_j)  = \operatorname{tr}(\Sigma) = \operatorname{tr}(V \; \Sigma_X \; V^T ) = \operatorname{tr}(\Sigma_X) = \sum_{j=1}^p \operatorname{Var}(Y_j) = \sum_{j=1}^p \lambda_j \]</span></p>
<p>Thus we can define the proportion of variance captured by the <span class="math inline">\(k\)</span>th principal component as</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \text{Proporition of variance due }  = \frac{\lambda_k}{\sum_{j=1}^p \lambda_j}, \quad k= 1,...,p \\
&amp; \text{to the $k$th principal component }
\end{aligned}
\]</span></p>
<p>The basic idea behind PCA for dimensionality reduction is to only keep the the first <span class="math inline">\(k\)</span> principal components which captures, say <span class="math inline">\(90\%\)</span>, of the variance, thereby reducing the number of variables.</p>
<p>Another way to select the principal components is to choose the most variables possible such that the conditon number (ratio of largest to smallest eigenvalues) of the reduced covariance matrix is close to <span class="math inline">\(1\)</span>. This is because machine learning algorithms, like linear and logistic regression, preform better (more precision and faster convergence for gradient based methods) when the condition number of the covariance matrix is close to <span class="math inline">\(1\)</span>.</p>
<section id="population-pca-using-the-correlation-matrix" class="level3">
<h3 class="anchored" data-anchor-id="population-pca-using-the-correlation-matrix">Population PCA using the correlation matrix</h3>
<p>PCA can also be calculated using the correlation matrix <span class="math inline">\(P\)</span> which is the covariance matrix of the standardized features having their mean set to <span class="math inline">\(0\)</span> and variance to <span class="math inline">\(1\)</span>. Specifically, one applies the above calculations to the following variables:</p>
<p><span class="math display">\[
\begin{aligned}
    Z_1 &amp;= \frac{X_1 - \mu_1}{\sqrt{\sigma_{11}}} \\
    Z_2 &amp;= \frac{X_2 - \mu_2}{\sqrt{\sigma_{22}}} \\
    \vdots &amp; \quad \vdots \\
    Z_p &amp;= \frac{X_p - \mu_p}{\sqrt{\sigma_{pp}}}
\end{aligned}
\]</span></p>
<p>In matrix notation we have</p>
<p><span class="math display">\[
\begin{aligned}
\vec{Z} &amp;= (\vec{W}^{\frac{1}{2}})^{-1}(\vec{X} - \vec{\mu}) \\
\operatorname{cov}[\vec{Z}] &amp; = (\vec{W}^{\frac{1}{2}})^{-1} \; \Sigma \; (\vec{W}^{\frac{1}{2}})^{-1} = P
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\vec{W} = \operatorname{diag}(\sigma_{11},...,\sigma_{pp})\)</span>. Then one obtains a decomposition similar to above with the following difference in the formula:</p>
<p><span class="math display">\[
\sum_{j=1}^p \operatorname{Var}(Y_j) = \sum_{j=1}^p \operatorname{Var}(X_j) = p
\]</span></p>
<p>Then the formula for the proportion of variance captured by the <span class="math inline">\(k\)</span>th principal component simplifies to:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \text{Proporition of variance due }  = \frac{\lambda_k}{p}, \quad k= 1,...,p \\
&amp; \text{to the $k$th principal component }
\end{aligned}
\]</span></p>
<p>It’s important to note that the principal components calculated from <span class="math inline">\(\Sigma\)</span> and <span class="math inline">\(P\)</span> can differ significantly, being a different linear combination of the original variables <span class="citation" data-cites="Johnson1982AppliedMS">(see <a href="#ref-Johnson1982AppliedMS" role="doc-biblioref">Johnson and Wichern 1982</a> Example 8.2)</span>. The principal components are similar when the variances for the <span class="math inline">\(p\)</span> random variables are the same, <span class="math inline">\(\sigma = \sigma_{11} = \sigma_{22} = \dots = \sigma_{pp}\)</span>. Finally note that it’s <strong>good practise to work with variables on the same scales</strong> so that the principal components aren’t heavily skewed towards the larger variables.</p>
</section>
</section>
<section id="sample-pca" class="level2">
<h2 class="anchored" data-anchor-id="sample-pca">Sample PCA</h2>
<p>Consider <span class="math inline">\(n\)</span> iid realizations <span class="math inline">\(\vec{X}_1, ..., \vec{X}_n\)</span> from the random vector <span class="math inline">\(\vec{X}\)</span>. We can arrange this data into a feature matrix</p>
<p><span class="math display">\[
X =  \begin{bmatrix}
\vec{X}_1 \\
\vdots \\
\vec{X}_n \\
\end{bmatrix}
\]</span></p>
<p>The sample covariance matrix, <span class="math inline">\(S\)</span>, is:</p>
<p><span class="math display">\[
\begin{aligned}
S_{i j}  = \frac{1}{n-1} \sum_{k=1}^n (X_{k i} - \bar{X}_i) (X_{k j} - \bar{X}_j) = \frac{1}{n-1} (X - \vec{1} \otimes\bar{X})^T (X - \bar{X} \otimes \vec{1})
\end{aligned} \quad \quad i= 1,...,p \text{ and } j = 1,...,p\]</span></p>
<p>and the sample correlation matrix, <span class="math inline">\(R\)</span>, is:</p>
<p><span class="math display">\[
\begin{aligned}
R_{ij} = \frac{S_{ij}}{\sqrt{S_{ii}} \sqrt{S_{jj}}} = \frac{\sum_{k=1}^n (X_{k i} - \bar{X}_i) (X_{k j} - \bar{X}_j)}{\sqrt{\sum_{k=1}^n (X_{k i} - \bar{X}_i)^2 } \sqrt{\sum_{k=1}^n (X_{k j} - \bar{X}_j)^2}}
\end{aligned} \quad \quad i= 1,...,p \text{ and } j = 1,...,p
\]</span></p>
<p>For <span class="math inline">\(\vec{v} \in \mathbb{R}^p\)</span> a new feature is defined by the equation</p>
<p><span class="math display">\[ Y = \vec{X} \cdot \vec{v} \]</span></p>
<p>Formulas similar to above show that the sample covariance matrix transforms according to <span class="citation" data-cites="Johnson1982AppliedMS">(see chapter 8 in <a href="#ref-Johnson1982AppliedMS" role="doc-biblioref">Johnson and Wichern 1982</a>)</span>:</p>
<p><span class="math display">\[ S_Y = V S_X V^T \]</span></p>
<p>Thus an identical construction can be given for the principal components, now using <span class="math inline">\(S\)</span> in place of <span class="math inline">\(\Sigma\)</span>, or <span class="math inline">\(R\)</span> in place of <span class="math inline">\(P\)</span>.</p>
<section id="reconstruction-error" class="level4">
<h4 class="anchored" data-anchor-id="reconstruction-error">Reconstruction error</h4>
<p>Let <span class="math inline">\(W_d = [ v_1 \dots v_d ]\)</span> be the matrix whose columns are the first <span class="math inline">\(d\)</span> eigenvectors of <span class="math inline">\(S_X\)</span> (ordered from largest to smallest eigenvalue). Then given any feature vector <span class="math inline">\(\vec{X}_i\)</span>, the vector <span class="math inline">\(\vec{X}_i W_d\)</span> is the <span class="math inline">\(d\)</span> dimensional projection, and the vector <span class="math inline">\(\vec{X}_i W_d W_d^T\)</span> is the reconstructed vector. There’s a fairly simple formula for the reconstruction error</p>
<p><span class="math display">\[
\sum_{i=1}^n || \vec{X}_i W_d W_d^T - \vec{X}_i||^2
\]</span></p>
<p>It turns out that <span class="citation" data-cites="Johnson1982AppliedMS">(see chapter 9 in <a href="#ref-Johnson1982AppliedMS" role="doc-biblioref">Johnson and Wichern 1982</a> for a proof)</span></p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^n || \vec{X}_i W_d W_d^T - \vec{X}_i||^2 &amp; = \operatorname{tr}[(X W_d W_d^T - X)(X W_d W_d^T - X)^T] \\
&amp;  \vdots \\
&amp; = \lambda_{d+1} + \dots + \lambda_p
\end{aligned}
\]</span></p>
<p>Thus the recontruction error of the <span class="math inline">\(d\)</span> dimensional projection is equal to the remaining <span class="math inline">\(p-d\)</span> eigenvalues of the covariance matrix, i.e.&nbsp;the unexplained variance. Finally, note that this is the smallest possible error among all <span class="math inline">\(d\)</span> dimensional projections, and this is another defining property of PCA.</p>
</section>
</section>
<section id="large-sample-properties" class="level2">
<h2 class="anchored" data-anchor-id="large-sample-properties">Large sample properties</h2>
<p>As expected, when the feature matrix <span class="math inline">\(X\)</span> comes from a multivariate Gaussian, the principal components recover the natural frame which diagonalizes the covariance matrix of the Gaussian. In fact, much more can be said.</p>
<p>When <span class="math inline">\(\vec{X}_i \overset{iid}{\sim} N(\vec{\mu}, \vec{\Sigma})\)</span> and the eigenvalues of <span class="math inline">\(\Sigma\)</span> are strictly positive and unique: <span class="math inline">\(\lambda_1 &gt; \dots \lambda_p &gt; 0\)</span>, we can describe the large sample properties of the principal directions and eigenvalues. Then as <span class="math inline">\(n \rightarrow \infty\)</span>, we have <span class="citation" data-cites="Johnson1982AppliedMS">(<a href="#ref-Johnson1982AppliedMS" role="doc-biblioref">Johnson and Wichern 1982</a>)</span>:</p>
<p><span class="math display">\[
    \sqrt{n}(\hat{\vec{\lambda}} - \vec{\lambda}) \approx N(\vec{0}, 2 \vec{\Lambda}^2) \\
    \sqrt{n}(\hat{\vec{v}}_k - \vec{v}_k) \approx N(\vec{0}, \vec{V}_k)
\]</span></p>
<p>where <span class="math inline">\(\Lambda = \operatorname{diag}(\lambda_1, ..., \lambda_k)\)</span> and</p>
<p><span class="math display">\[
    \vec{V}_k = \lambda_k \sum_{l \neq k} \frac{\lambda_l}{(\lambda_l - \lambda_k)^2} \vec{v}_l^T \vec{v}_l
\]</span></p>
<p>Furthermore, as <span class="math inline">\(n \rightarrow \infty\)</span>, we have that <span class="math inline">\(\hat{\lambda}_k\)</span> and <span class="math inline">\(\hat{\vec{v}}_k\)</span> are independent.</p>
</section>
<section id="practical-considerations" class="level2">
<h2 class="anchored" data-anchor-id="practical-considerations">Practical considerations</h2>
<p>Below we follow <a href="https://github.com/ageron/handson-ml2/blob/master/08_dimensionality_reduction.ipynb">A. Geron’s notes on PCA</a> to implement it in numpy and sckit-learn for the iris dataset.</p>
<div id="1f9809f4" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>iris <span class="op">=</span> datasets.load_iris()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fadb4892" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(iris[<span class="st">'feature_names'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']</code></pre>
</div>
</div>
<div id="e7331ff7" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(iris[<span class="st">'data'</span>], columns<span class="op">=</span>[<span class="st">'sepal_length'</span>, <span class="st">'sepal_width'</span>, <span class="st">'petal_length'</span>, <span class="st">'petal_width'</span>])</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>df.head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>5.1</td>
<td>3.5</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4.9</td>
<td>3.0</td>
<td>1.4</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4.7</td>
<td>3.2</td>
<td>1.3</td>
<td>0.2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>4.6</td>
<td>3.1</td>
<td>1.5</td>
<td>0.2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5.0</td>
<td>3.6</td>
<td>1.4</td>
<td>0.2</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="627cff12" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df.describe()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">sepal_length</th>
<th data-quarto-table-cell-role="th">sepal_width</th>
<th data-quarto-table-cell-role="th">petal_length</th>
<th data-quarto-table-cell-role="th">petal_width</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>150.000000</td>
<td>150.000000</td>
<td>150.000000</td>
<td>150.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>5.843333</td>
<td>3.057333</td>
<td>3.758000</td>
<td>1.199333</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>0.828066</td>
<td>0.435866</td>
<td>1.765298</td>
<td>0.762238</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>4.300000</td>
<td>2.000000</td>
<td>1.000000</td>
<td>0.100000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>5.100000</td>
<td>2.800000</td>
<td>1.600000</td>
<td>0.300000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>5.800000</td>
<td>3.000000</td>
<td>4.350000</td>
<td>1.300000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>6.400000</td>
<td>3.300000</td>
<td>5.100000</td>
<td>1.800000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>7.900000</td>
<td>4.400000</td>
<td>6.900000</td>
<td>2.500000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Since the units of each feature are comparable (the standard deviations are of the same order of magnitude), we will calculate PCA using the sample covariance matrix. If on the otherhand, the magnitudes differed by orders of magnitude, we would calculate PCA using the sample correlation matrix.</p>
<section id="pca-using-svd-directly" class="level3">
<h3 class="anchored" data-anchor-id="pca-using-svd-directly">PCA using SVD directly</h3>
<p>The principal components can be calculated by diagonalizing the covariance matrix or calulating the singular value decomposition of the feature matrix. We’ll use the latter for numerical reasons.</p>
<div id="cca2b311" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> iris[<span class="st">'data'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The SVD of <span class="math inline">\(X\)</span> is calculated after subtracting off the sample mean. We’ll call this matrix <span class="math inline">\(X_{centered}\)</span> and work with it hereafter. Then the SVD of <span class="math inline">\(X_{centered}\)</span> is</p>
<p><span class="math inline">\(X_{centered} = U D V^T\)</span></p>
<p>where <span class="math inline">\(U\)</span> is an <span class="math inline">\(n \times n\)</span> orthonormal matrix, <span class="math inline">\(V\)</span> is an <span class="math inline">\(m \times m\)</span> orthonormal matrix, and <span class="math inline">\(D = \operatorname{diag}(d_{11}, ..., d_{mm})\)</span> is an <span class="math inline">\(n \times m\)</span> rectangular diagonal matrix.</p>
<p>The matrix <span class="math inline">\(V\)</span> is the matrix of eigenvectors from the sample PCA above, and the diagonal covariance matrix <span class="math inline">\(S_Y\)</span> is obtained by squaring the singular values and dividing by <span class="math inline">\(n-1\)</span>:</p>
<p><span class="math display">\[
S_Y =   \frac{1}{n-1}\begin{bmatrix}
    d_{11}^2 &amp; &amp; \\
    &amp; \ddots &amp; \\
    &amp; &amp; d_{mm}^2
  \end{bmatrix}
\]</span></p>
<p>For this example, we have:</p>
<div id="594e1f20" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Direct computation using SVD</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>X_centered <span class="op">=</span> X <span class="op">-</span> X.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>U, D, Vt <span class="op">=</span> np.linalg.svd(X_centered)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> Vt.T</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>eigs <span class="op">=</span> D <span class="op">**</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <span class="math inline">\(d\)</span>-dimensional projection can be calcualted using the <span class="math inline">\(p \times d\)</span> matrix <span class="math inline">\(W_d\)</span> whose <span class="math inline">\(d\)</span> columns are the first <span class="math inline">\(d\)</span> eigenvectors.</p>
<p><span class="math inline">\(X_{d-proj} = X_{centered}    W_d\)</span></p>
<div id="fa4369e7" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> V[:, :<span class="dv">2</span>]</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># W2 can also be obtained from the sklearn package using: pca.components_.T </span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>X2D_diy <span class="op">=</span> X_centered <span class="op">@</span> W2</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>X2D_diy.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>(150, 2)</code></pre>
</div>
</div>
<p>Obtain the reconstructed data using:</p>
<p>$ X_{recovered} = X_{d-proj} W_{d}^T$</p>
<p>A defining property of PCA is that for any dimension <span class="math inline">\(d &lt; m\)</span>, the PCA reconstruction of the data minimizes the mean squared error between the original data among all possible <span class="math inline">\(d\)</span> dimensional hyperplanes. Moreover, the square of this error is equal to the sum of the <span class="math inline">\(m-d\)</span> smaller eigenvalues, as we shall see.</p>
<div id="1dfafa5d" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>X_recovered <span class="op">=</span> X2D_diy <span class="op">@</span> W2.T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The squared error is then:</p>
<div id="66e8b210" class="cell" data-scrolled="true" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>np.linalg.norm(X_centered <span class="op">-</span> X_recovered)<span class="op">**</span><span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>15.204644359438948</code></pre>
</div>
</div>
<p>Which is equal to the sum of the two smaller eigenvalues:</p>
<div id="f915beda" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">sum</span>(eigs[<span class="dv">2</span>:])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>15.204644359438953</code></pre>
</div>
</div>
<p>The proportion of the total sample variance due to each sample principal component is:</p>
<p><span class="math display">\[
\frac{\hat{\lambda_i}}{(\sum_{i=1}^m \hat{\lambda_i}) } \quad \quad \quad i = 1,...,m
\]</span></p>
<p>and is given numerically by</p>
<div id="52ce5990" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>eigs<span class="op">/</span><span class="bu">sum</span>(eigs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>array([0.92461872, 0.05306648, 0.01710261, 0.00521218])</code></pre>
</div>
</div>
<p>We can obtain the number of principal components to retain by either (i) retaining the first <span class="math inline">\(d\)</span> components which sum up to 95% of the total variance, or (ii) looking for an elbow in the scree plot.</p>
<div id="d1a77ec8" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>,<span class="bu">len</span>(eigs)<span class="op">+</span><span class="dv">1</span>), np.array(eigs.tolist()))</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>plt.title(label<span class="op">=</span><span class="st">"Scree plot"</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"i"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"lambda_i"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-03-30-principal-component-analysis_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>There is an elbow in the plot at <span class="math inline">\(i = 2\)</span>, meaning all eigenvalues after <span class="math inline">\(\lambda_1\)</span> are relatively small and about the same size. In this case, it appears that the first principal component effectively summarize the total sample variance.</p>
<p>Finally, we can visualize the two dimensional projection using a scatter plot.</p>
<div id="96c636ca" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"PCA with 2 principal components"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(X2D_diy[:, <span class="dv">0</span>], X2D_diy[:,<span class="dv">1</span>], c<span class="op">=</span>iris[<span class="st">'target'</span>])</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$y_1$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y_2$"</span>, fontsize<span class="op">=</span><span class="dv">18</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2022-03-30-principal-component-analysis_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="pca-using-scikit-learn" class="level3">
<h3 class="anchored" data-anchor-id="pca-using-scikit-learn">PCA using scikit-learn</h3>
<p>Now we’ll calculate the PCA projection using scikit-learn. Notice that scikit-learn will mean center the data itself.</p>
<div id="ab3bf4f5" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components <span class="op">=</span> <span class="dv">2</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>X2D <span class="op">=</span> pca.fit_transform(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Instead of specifing the number of components in the call to PCA above, we can also set <span class="math inline">\(\operatorname{n\_components}=0.95\)</span> to obtain the number of dimensions which capture 95% of the variance in the data. Or we could pass in no arguments to get the full SVD and plot a scree plot.</p>
<p>Below, we check that sklearn PCA and the one calculated with SVD directly give the same answer.</p>
<div id="079e0f49" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">max</span>(X2D <span class="op">-</span> X2D_diy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>2.748330173586095</code></pre>
</div>
</div>
<p>The non-uniquness of the PCA leads to a large difference. However, this can be fixed by inspecting the projections and adjusting:</p>
<div id="2da4b26f" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>X2D_diy[:,<span class="dv">1</span>] <span class="op">=</span> <span class="op">-</span> X2D_diy[:,<span class="dv">1</span>]</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">max</span>(X2D <span class="op">-</span> X2D_diy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>1.3322676295501878e-15</code></pre>
</div>
</div>
<p>The explained variance ratio holds the proportion of variance explained by the first two sample principal components, and is given by</p>
<div id="c608cec8" class="cell" data-scrolled="true" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>pca.explained_variance_ratio_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>array([0.92461872, 0.05306648])</code></pre>
</div>
</div>
<section id="computational-complexity" class="level4">
<h4 class="anchored" data-anchor-id="computational-complexity">Computational complexity</h4>
<p>Let <span class="math inline">\(n_{max} = \operatorname{max}(m,n)\)</span> and <span class="math inline">\(n_{min} = \operatorname{min}(m,n)\)</span>. If <span class="math inline">\(n_{max} &lt; 500\)</span> and <span class="math inline">\(d\)</span> is less than <span class="math inline">\(80\%\)</span> of <span class="math inline">\(n_{min}\)</span>, then scikit-learn uses the full SVD approach which has a complexity of <span class="math inline">\(O(n_{max}^2 n_{min})\)</span>, otherwise it uses randomized PCA which has a complexity of <span class="math inline">\(O(n_{max}^2 d)\)</span> (cf.&nbsp;<a href="https://scikit-learn.org/stable/modules/decomposition.html#pca-using-randomized-svd">here</a> and <span class="citation" data-cites="Gron2017HandsOnML">(<a href="#ref-Gron2017HandsOnML" role="doc-biblioref">Géron 2017</a>)</span>).</p>
<p>There are many more variations of PCA, and other dimensionality reduction algorithms. See <a href="https://scikit-learn.org/stable/modules/decomposition.html#decomposing-signals-in-components-matrix-factorization-problems">Decomposing signals in components (matrix factorization problems)</a> in the scikit-learn documentation, for example.</p>



</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Gron2017HandsOnML" class="csl-entry" role="listitem">
Géron, Aurélien. 2017. <span>“Hands-on Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems.”</span> In.
</div>
<div id="ref-Johnson1982AppliedMS" class="csl-entry" role="listitem">
Johnson, Richard A., and Dean W. Wichern. 1982. <span>“Applied Multivariate Statistical Analysis.”</span> In.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/krishanr\.github\.io\/blog\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>