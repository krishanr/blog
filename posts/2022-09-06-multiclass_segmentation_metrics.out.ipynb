{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multiclass Segmentation Metrics\n",
        "\n",
        "In my last [post](2022-08-25-pet_segmentation.html) I showed how to use\n",
        "`torchmetrics` to implement segmentation metrics for the Oxford-IIIT pet\n",
        "segmentation dataset. We saw that in addition to the `average` keyword\n",
        "introduced in the [pet breed\n",
        "classification](2022-08-17-pet_breed_classification.html) post, the\n",
        "`mdmc_average` keyword is necessary to compute metrics for image data.\n",
        "\n",
        "In this post we’ll dive deeper into these metrics, explaining the two\n",
        "choices for the `mdmc_average` parameter, including *global* and\n",
        "*samplewise*, as well as giving recommendations for dealing with\n",
        "imbalanced datasets.\n",
        "\n",
        "The examples below will look primarily at precision and $F1$ score, but\n",
        "note that these metrics can be replaced by recall, dice score, etc."
      ],
      "id": "701508af-9a08-4bad-8ab8-482b845f287f"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install -U git+https://github.com/qubvel/segmentation_models.pytorch\n",
        "!pip install seaborn\n",
        "!pip install watermark"
      ],
      "id": "cfb7068d"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import functools\n",
        "import segmentation_models_pytorch as smp\n",
        "from torchmetrics.functional.classification import precision, f1_score\n",
        "from torchmetrics.classification import StatScores\n",
        "from sklearn import metrics\n",
        "\n",
        "# Set the seed for reproduciblity.\n",
        "torch.manual_seed(7)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap"
      ],
      "id": "595a4bfe"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To better understand the metrics, we’ll work with a $4$ class problem\n",
        "with $n = 100$ samples. Classes $0$ and $3$ will have a probability of\n",
        "occurence of $\\frac{1}{15}$, class $1$ will have a probability of\n",
        "$\\frac{2}{3}$, and class $2$ will have a probability of $\\frac{1}{5}$.\n",
        "We can generate data having this distribution using `torch.multinomial`\n",
        "below."
      ],
      "id": "56e81250-cfe4-4be9-a65c-08c25ea20462"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights = torch.tensor([1, 10, 3, 1], dtype=torch.float)\n",
        "num_classes = len(weights)\n",
        "shape = (100,1,256,256)\n",
        "size = functools.reduce(lambda x, y : x* y, shape)\n",
        "output = torch.multinomial(weights, size, replacement=True).reshape(shape)\n",
        "output[70:,:,:,:] = torch.zeros(30, *shape[1:])\n",
        "target = torch.multinomial(weights, size, replacement=True).reshape(shape)\n",
        "target[70:,:,:,:] = torch.zeros(30, *shape[1:])"
      ],
      "id": "7513e786"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example, a subset of the output looks like:"
      ],
      "id": "74db42ba-e625-4276-8c7a-d3be2d02bc25"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "outputId": "c67b961c-45ac-46f2-ad3a-f2f52b2c20b0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[[1, 1, 2, 1, 2, 2, 1, 1, 2, 2],\n",
              "         [1, 1, 1, 1, 1, 3, 1, 1, 1, 1],\n",
              "         [1, 1, 3, 2, 1, 2, 1, 1, 1, 1],\n",
              "         [0, 1, 1, 2, 3, 1, 1, 1, 1, 2],\n",
              "         [1, 0, 1, 1, 1, 1, 1, 1, 1, 3],\n",
              "         [1, 1, 1, 2, 0, 1, 1, 0, 1, 1],\n",
              "         [1, 1, 1, 0, 1, 1, 2, 1, 2, 1],\n",
              "         [2, 1, 1, 1, 2, 1, 2, 1, 3, 2],\n",
              "         [3, 1, 1, 3, 1, 2, 1, 1, 1, 1],\n",
              "         [2, 3, 0, 1, 1, 1, 1, 2, 2, 1]]])"
            ]
          }
        }
      ],
      "source": [
        "output[0,:,:10,:10]"
      ],
      "id": "4c483999"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we can collapse the image dimensions, $H$ and $W$, and then\n",
        "calculate metrics as for multiclass classification. This is precisely\n",
        "what happens when we choose `mdmc_average` global."
      ],
      "id": "790ed2d2-dcba-456d-b9a0-6ea666354009"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "0.4517214596271515"
            ]
          }
        }
      ],
      "source": [
        "precision(output, target,num_classes=num_classes,average=\"macro\",mdmc_average=\"global\").item()"
      ],
      "id": "990bda74-82a3-44f0-b725-9112ea26e661"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For comparisons sake, in scikit-learn we have:"
      ],
      "id": "b0d0d499-5f46-4386-b40d-68130f1e084e"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "0.4517214441963613"
            ]
          }
        }
      ],
      "source": [
        "metrics.precision_score(target.reshape((-1)).numpy(),output.reshape((-1)).numpy(), average=\"macro\")"
      ],
      "id": "58bedb70-9266-4f27-bf53-99741f1fad29"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then the different options for `average` can be chosen, including\n",
        "**micro**, **macro**, and **weighted**.\n",
        "\n",
        "In contrast, the image dimensions can be treated separately, which is\n",
        "called the **macro-imagewise** reduction:\n",
        "\n",
        "1.  For each image and class the confusion table is computed over all\n",
        "    pixels in an image.\n",
        "2.  Then the metric is computed for each image and class, as if it were\n",
        "    a binary classifier.\n",
        "3.  The metrics are finally averaged over the images and classes.\n",
        "\n",
        "This is the most natural way to calculate metrics like the Jaccard index\n",
        "(intersection over union) for example. Unfortunately the jaccard index\n",
        "can’t be calculated this way using `torchmetrics`. However the $F1$/Dice\n",
        "Score can be calculated using `torchmetrics`, and it’s equivalent to the\n",
        "Jaccard index[1]:\n",
        "\n",
        "[1] ‘This fact is discussed further\n",
        "[here](https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou)’"
      ],
      "id": "9b0b0d2b-dc26-4409-a759-9241c468cfee"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "0.2497853934764862"
            ]
          }
        }
      ],
      "source": [
        "f1_score(output, target,num_classes=num_classes,average=\"macro\",mdmc_average=\"samplewise\").item()"
      ],
      "id": "97309288-d336-489e-930b-62adfa09c74f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However if we calculate the $F1$ score using the segmentation models\n",
        "library, we get:"
      ],
      "id": "484fd83b-f6f7-4d64-b11c-5493ed2acc6f"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "0.47478538751602173"
            ]
          }
        }
      ],
      "source": [
        "tp, fp, fn, tn = smp.metrics.get_stats(output.long(), target.long(), mode='multiclass', num_classes=num_classes)\n",
        "smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"macro-imagewise\").item()"
      ],
      "id": "37a3b901-7b05-4b02-9ad9-a1a39b9ef376"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is because our dataset has many images with no targets (recall that\n",
        "we zeroed out several images). Thus the $F1$ score for non-background\n",
        "classes reduces to $\\frac{0}{0}$. `smp` replaces occurences of\n",
        "$\\frac{0}{0}$ by $1$, while `torchmetrics` replaces $\\frac{0}{0}$ by\n",
        "$0$. If we pass `zero_division=0` to the segmentation models library, we\n",
        "get the same value as `torchmetrics`:"
      ],
      "id": "a2e93138-a4e9-4c75-a167-5cabd52a1dce"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "0.2497853934764862"
            ]
          }
        }
      ],
      "source": [
        "tp, fp, fn, tn = smp.metrics.get_stats(output.long(), target.long(), mode='multiclass', num_classes=num_classes)\n",
        "smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"macro-imagewise\", zero_division=0).item()"
      ],
      "id": "19f9150c-9e93-4012-8dc9-258d7efb4572"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This we why we recommend avoiding `mdmc_average` equal to samplewise,\n",
        "and calculating the metrics like for regular multiclass classifiers\n",
        "instead.\n",
        "\n",
        "In conclusion when dealing with balanaced datasets, accuracy using the\n",
        "micro average plus `mdmc_average` global is sufficient, while the $F1$\n",
        "score with the weighted average plus `mdmc_average` global is more\n",
        "accurate for imbalanaced datasets."
      ],
      "id": "080a1701-085d-4c28-9980-f978ae51860c"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seaborn                    : 0.11.2\n",
            "matplotlib                 : 3.5.1\n",
            "segmentation_models_pytorch: 0.3.0\n",
            "torch                      : 1.12.0\n",
            "sklearn                    : 1.0.2\n"
          ]
        }
      ],
      "source": [
        "%load_ext watermark\n",
        "%watermark --iversions"
      ],
      "id": "9829ff05"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n",
        "\n",
        "-   [Torchmetrics\n",
        "    Quickstart](https://torchmetrics.readthedocs.io/en/stable/pages/quickstart.html)\n",
        "-   [Multiclass and multilabel classification in\n",
        "    scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification)\n",
        "-   [Segmentation Models Pytorch\n",
        "    Metrics](https://smp.readthedocs.io/en/latest/metrics.html)"
      ],
      "id": "9df4001b-d348-4375-b2c8-7acd02a8d5da"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pet_breed_classification_post.ipynb",
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  }
}